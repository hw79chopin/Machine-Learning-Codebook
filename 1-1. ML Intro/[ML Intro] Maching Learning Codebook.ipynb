{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(ì›ë³¸) [ML Intro] Maching Learning Codebook",
      "provenance": [],
      "collapsed_sections": [
        "sw5d6I-YbImk",
        "Q5DgX7QHcx91",
        "zNswN79X-4XH",
        "0hZLHHGP-4XI",
        "Eu9wklSp-4XJ",
        "eG8NJqwv-4XK",
        "E-zlueslZ7fe",
        "DSRYORPDZ7uv",
        "6aGwgB_wbCrO",
        "s61epZtz_JRI",
        "11nm8K92_JRV",
        "kkchB18abGyc",
        "uHVla3OfbG2m",
        "jiAATuLcbG-b",
        "61q0ZkG2bHJx",
        "-1SG_RNbIx5h",
        "TlmT50pyI2Df",
        "lL1W4FCFbND6",
        "dXxqASE8bNTj",
        "aBQ_pbhmbTXV",
        "bJWmUSDlAyOK"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYFuejL5sXMt"
      },
      "source": [
        "---\n",
        "# ğŸ“ Hyun's Code collection (Machine Learning Intro) \n",
        "---\n",
        "\n",
        "- What is Machine Learning?\n",
        "- Essential Concepts\n",
        "- Types of Machine Learning\n",
        "- Overall Process of Machine Learning Project\n",
        "- Extra Tips\n",
        "\n",
        "### <h3 align=\"right\">ğŸ¥‡ Authored by <strong>Hyun</strong></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw5d6I-YbImk"
      },
      "source": [
        "# âœï¸ What is **Machine Learning**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpmsSftm2QQq"
      },
      "source": [
        "![](https://lh3.googleusercontent.com/HK5QOxzeXCF0CMWSTae2OkxXmxETt0lNlyTYrYhVTiNrGxFr33lLgCnjfGiSSQgVVmJhjlkx4gM8eo71BBV6HW_IAo1b-kzqJL4ZLcqeiUhXIYin0vjeTjxrJOQWRmNSve6TX-er2RQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2wWk6km_CWp"
      },
      "source": [
        "- ë¨¸ì‹ ëŸ¬ë‹ì€ ê°„ë‹¨í•˜ê²Œ ë§í•´ì„œ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ ë§Œë“œëŠ” ê²ƒ\n",
        "- í•™ìŠµì´ë€? ì–´ë–¤ ì‘ì—…ì—ì„œ ì£¼ì–´ì§„ ì„±ëŠ¥ ì§€í‘œê°€ ë” ë‚˜ì•„ì§€ëŠ” ê²ƒì„ ì˜ë¯¸í•¨!\n",
        "\n",
        "- ê¸°ì¡´ **ë°ì´í„°ë¥¼ ë°”íƒ•**ìœ¼ë¡œ **ê¸°ê³„ê°€ í•™ìŠµ**í•˜ëŠ” ê²ƒì„ ì˜ë¯¸\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5DgX7QHcx91"
      },
      "source": [
        "# âœï¸ Essential Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNswN79X-4XH"
      },
      "source": [
        "## ğŸ” Bias vs Variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr8EG5Vb-4XI"
      },
      "source": [
        "\n",
        "\n",
        "![bias](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile9.uf.tistory.com%2Fimage%2F99CDCC33599AC28F075E3C)\n",
        "\n",
        "> Biasë€?\n",
        " - BiasëŠ” ì‹¤ì œ ê°’ì—ì„œ ë©€ì–´ì§„ ì²™ë„ \n",
        "\n",
        "> Varianceë€?\n",
        " - VarianceëŠ” ì˜ˆì¸¡ëœ ê°’ë“¤ì´ ì„œë¡œ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ì— ëŒ€í•œ ì²™ë„\n",
        "\n",
        "> Machine Learning Modelì˜ ëª©í‘œ\n",
        " - ì™¼ìª½ ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ Biasë„ ë‚®ê³ , Varianceë„ ë‚®ê²Œ, ì¦‰ ëª¨ë‘ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hZLHHGP-4XI"
      },
      "source": [
        "## ğŸ” Cost function, Loss function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wjWMlkN-4XI"
      },
      "source": [
        "> Loss function\n",
        "- Loss functionì€ data pointì—ì„œì˜ ì˜¤ì°¨\n",
        "- Single data setì—ì„œì˜ ì˜¤ì°¨ë¥¼ êµ¬í•˜ëŠ” ê²ƒ\n",
        "ìˆœê°„ìˆœê°„ì˜ lossë¥¼ íŒë‹¨í•  ë• loss function  \n",
        "  í•™ìŠµì´ ì™„ë£Œëœ í›„ì—ëŠ” cost function!\n",
        "\n",
        "\n",
        "> Cost function\n",
        "- Cost functionì€ loss functionì˜ í•©ì´ë‹¤. \n",
        "- ì¦‰ entire data setì—ì„œ loss funcitonì„ ê³„ì‚°í•œ í•©ì´ë‹¤.\n",
        "- ìˆœê°„ìˆœê°„ì˜ lossë¥¼ íŒë‹¨í•  ë• loss function, í•™ìŠµì´ ì™„ë£Œëœ í›„ì—ëŠ” cost function!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu9wklSp-4XJ"
      },
      "source": [
        "## ğŸ” Overfitting, Underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyLAT0EN-4XJ"
      },
      "source": [
        "![datasets](https://t1.daumcdn.net/cfile/tistory/9951E5445AAE1BE025)\n",
        "<br>\n",
        "\n",
        "![123123](https://mblogthumb-phinf.pstatic.net/MjAxODA3MzBfMjMy/MDAxNTMyODkwNjUxMjY4.H_ocFIRFaG8MWrBsv8BWrTCaAMGLMKZZUh_Rd1krRLog.HAZRdDtrQMvVGKiEWfGls8bm0EhTyRKf7XzoSY1Cibsg.JPEG.qbxlvnf11/maxresdefault.jpg?type=w800)\n",
        "\n",
        "> **ê³¼ëŒ€ì í•©(Overfitting)**ì´ë€?\n",
        " - ëª¨ë¸ì´ Train Datasetì— ë„ˆë¬´ ì˜ ë§ì•„ì„œ ì¼ë°˜ì„±ì´ ë–¨ì–´ì§€ê²Œ ë˜ëŠ” ë¬¸ì œ\n",
        " - ì¦‰ Train Datasetì„ ë„ˆë¬´ ê³¼í•˜ê²Œ í•™ìŠµí•´ í•™ìŠµë˜ì§€ ì•Šì€ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ë¶„ë¥˜í•˜ì§€ ëª»í•˜ê²Œ ë˜ëŠ” ê²ƒ\n",
        " - ìœ„ ê·¸ë¦¼ì˜ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ë“¤ì´ ê³¼ëŒ€ì í•©ì˜ ì˜ˆì‹œ\n",
        " - ë³´ì‹œë‹¤ì‹œí”¼ Train Setì„ ê±°ì˜ ë‹¤ ê±°ì¹˜ê±°ë‚˜ ë¶„ë¥˜í•´ë‚´ë©° êµ‰ì¥íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆì§€ë§Œ, ìƒˆë¡œìš´ ë³€ìˆ˜ì— ëŒ€ì‘í•˜ê¸° ì–´ë ¤ì›€\n",
        " - ê·¸ë¦¼ì˜ ê°€ìš´ë° ëª¨ë¸ì´ ì í•©í•œ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆìŒ!\n",
        "\n",
        "> **ê³¼ì†Œì í•©(Underfitting)**ì´ë€?\n",
        " - Overfittingê³¼ ë°˜ëŒ€ ê°œë…\n",
        " - ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ ë°ì´í„°ì˜ ë‚´ì¬ëœ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ê²ƒ\n",
        " - ìœ„ ê·¸ë¦¼ì˜ ì™¼ìª½ ê·¸ë¦¼ì´ underfitting.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMvaT_ZRiXGA"
      },
      "source": [
        "\n",
        "\n",
        "> Machine Learningì—ì„œ **Overfittingì„ ë°©ì§€í•˜ëŠ” ë°©ë²•**\n",
        " - ë” ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµí•˜ê¸°\n",
        " - Parameterê°€ ì ì€ ëª¨ë¸ì„ ì„ íƒ\n",
        " - Feature ê°œìˆ˜ ì¤„ì´ê¸°\n",
        " - ì •ê·œí™”(Regularization) ì‹œí‚¤ê¸° (e.g. L1, L2)  \n",
        "   ```1.2 Machine Learning ê´€ë ¨ ìš©ì–´ëª¨ìŒ``` í™•ì¸\n",
        " - Early stopping: ì–´ëŠ ì •ë„ lossê°€ ì¤„ì–´ë“¤ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ë©ˆì¶”ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji66dr3tiqQj"
      },
      "source": [
        "> Bias, Variance and fitting\n",
        "1. Overfitting ëª¨ë¸\n",
        " - High Variace ëª¨ë¸(ì˜¤ë¥¸ìª½ ìœ„)\n",
        " - Train Dataì˜ ì§€ì—½ì ì¸ íŠ¹ì„±ê¹Œì§€ ë°˜ì˜ë˜ì–´ í•™ìŠµëœ ê²ƒì€ ì˜ ì˜ˆì¸¡í•˜ì§€ë§Œ, í•™ìŠµë˜ì§€ ì•Šì€ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì˜ˆì¸¡ë ¥ì´ ë–¨ì–´ì§€ê²Œ ë¨\n",
        "2. Underfitting ëª¨ë¸\n",
        " - High Bias ëª¨ë¸(ì™¼ìª½ ì•„ë˜)\n",
        " - ë„ˆë¬´ ì ì€ íŠ¹ì„±ë§Œì„ ë°˜ì˜í•˜ì—¬, ì˜ˆì¸¡ì˜ ë²”ìœ„ê°€ ì¢ê³  ì •í™•ë„ê°€ ë–¨ì–´ì§€ê²Œ ë¨\n",
        "\n",
        "- ë‹¤ì‹œ ë§í•˜ë©´, Bias ì—ëŸ¬ê°€ ë†’ì•„ì§€ëŠ” ê²ƒì€ ë§ì€ ë°ì´í„°ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•„(=ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´) ì •í™•í•œ ì˜ˆì¸¡ì„ í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ë¥¼ ë§í•˜ê³ , Variance(ë¶„ì‚°) ì—ëŸ¬ëŠ” ë…¸ì´ì¦ˆê¹Œì§€ ì „ë¶€ í•™ìŠµí•´(=ëª¨ë¸ì´ ë„ˆë¬´ ë³µì¡í•´) ì•½ê°„ì˜ inputì—ë„ ì˜ˆì¸¡ Y ê°’ì´ í¬ê²Œ í”ë“¤ë¦¬ëŠ” ê²ƒì„ ë§í•¨. \n",
        "- ì´ ë‘ê°€ì§€ ì—ëŸ¬ê°€ **Trade-off** ê´€ê³„ì— ìˆì–´ì„œ ì´ ë‘˜ì„ ëª¨ë‘ ì¡ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥ í•œ ë”œë ˆë§ˆê°€ ìƒê¹€\n",
        " ![graph](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile9.uf.tistory.com%2Fimage%2F996DB433599AC34225B9BD)\n",
        "- Total Errorê°€ ìµœì†Œì¸ ì§€ì ì„ ì°¾ê¸° ìœ„í•œ ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•ì€ **Validation Setì„ ë§Œë“œëŠ” ê²ƒ**ì´ë‹¤.  \n",
        "(ë°‘ì—ì„œ Validation Set ë¶€ê°€ì„¤ëª…)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG8NJqwv-4XK"
      },
      "source": [
        "## ğŸ” Terminology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlGi0qnH-4XM"
      },
      "source": [
        "<h2> ğŸˆ Accuracy (ì •í™•ë„) </h2>\n",
        "\n",
        "\n",
        "- ì •í™•ë„(accuracy)ë€ ì „ì²´ ìƒ˜í”Œ ì¤‘ ë§ê²Œ ì˜ˆì¸¡í•œ ìƒ˜í”Œ ìˆ˜ì˜ ë¹„ìœ¨\n",
        "- ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨í˜•ì´ê³  ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµì—ì„œ ìµœì í™” ëª©ì í•¨ìˆ˜ë¡œ ì‚¬ìš©ë¨!!\n",
        "- ì •í™•ë„(Accuracy)ì˜ ê°€ì¥ í° ë¬¸ì œì ì€ **í´ë˜ìŠ¤ì˜ ë¶„í¬ê°€ ê°™ì„ ë•Œë§Œ ì´ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ì **\n",
        "- ì´ëŸ¬í•œ **ë‹¨ì ì„ ë³´ì™„**í•˜ëŠ” ì§€í‘œê°€ ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall), ROC ê³¡ì„ ê³¼ AUCì…ë‹ˆë‹¤.\n",
        "- $$accuracy=\\frac{TP+TN}{TP+TN+FP+FN}$$<br>\n",
        "``` python\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "accuracy_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "<h2> ğŸˆ Attribute (ì†ì„±) </h2>\n",
        "\n",
        "\n",
        "- ë°ì´í„° íƒ€ì…ì„ ì˜ë¯¸\n",
        "- Feature(íŠ¹ì„±)ì€ attribute + ê°’. í—ˆë‚˜ ë³´í†µ ë§ì€ ì‚¬ëŒì´ featureì™€ attributeë¥¼ êµ¬ë¶„í•˜ì§€ ì•Šê³  ì‚¬ìš©\n",
        "\n",
        "<h2> ğŸˆ AUC </h2>\n",
        "\n",
        "- AUCë€ ROC ê³¡ì„  ì•„ë˜ ë¶€ë¶„ì˜ ë©´ì ì´ë‹¤. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ìŒ!\n",
        "\n",
        "<h2> ğŸˆ Batch Learning <-> Online learning </h2>\n",
        "\n",
        "- ë°°ì¹˜ í•™ìŠµ!\n",
        "- ê°€ìš©í•œ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•´ í›ˆë ¨ì‹œì¼œì•¼ í•¨\n",
        "- ì‹œê°„ê³¼ ìì›ì„ ë§ì´ ì†Œëª¨\n",
        "- ë³´í†µ ì˜¤í”„ë¼ì¸ì—ì„œ ìˆ˜í–‰\n",
        "- ì‹œìŠ¤í…œì´ í›ˆë ¨ë˜ê³  ì ìš©ë˜ë©´ ë” ì´ìƒì˜ í•™ìŠµì—†ì´ ì‹¤í–‰ëœë‹¤.\n",
        "\n",
        "<h2> ğŸˆ Batch size </h2>\n",
        "\n",
        "- Batch size: dataë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒ\n",
        "\n",
        "<h2> ğŸˆ Bagging </h2>\n",
        "\n",
        "- Baggingì€ ìƒ˜í”Œì„ ì—¬ëŸ¬ ë²ˆ ë½‘ì•„ ê° ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ê²°ê³¼ë¥¼ ì§‘ê³„(Aggregating) í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n",
        "\n",
        "<h2> ğŸˆ Bias </h2>\n",
        "\n",
        " - BiasëŠ” ì‹¤ì œ ê°’ì—ì„œ ë©€ì–´ì§„ ì²™ë„ \n",
        "\n",
        "<h2> ğŸˆ Bootstrapping </h2>\n",
        "\n",
        " - í†µê³„í•™ì—ì„œ ê°€ì„¤ì„ testí•˜ê±°ë‚˜ metricì„ ê³„ì‚°í•˜ì§€ ì „ì— 'ì¤‘ë³µì„ í—ˆìš©í•œ' random samplingì„ ì ìš©í•˜ëŠ” ë°©ë²•\n",
        " - Machine learningì—ì„œ bootstrappingì´ë€ random samplingì„ í†µí•´ train dataë¥¼ ëŠ˜ë¦¬ëŠ” ë°©ë²•ì´ë‹¤!\n",
        " - Overfittingì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ëœë‹¤.\n",
        "\n",
        "<h2> ğŸˆ Cross validation </h2>\n",
        "\n",
        "- Cross validationì€ validation setì„ ë—„ ë§Œí¼ dataê°€ í¬ì§€ ì•Šì„ ë•Œ ì‚¬ìš©\n",
        "- Training Setì„ ì—¬ëŸ¬ Subsetìœ¼ë¡œ ë‚˜ëˆ„ê³  ê° ëª¨ë¸ì„ ì´ Subsetì˜ ì¡°í•©ìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ê³  ë‚˜ë¨¸ì§€ ë¶€ë¶„ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n",
        "\n",
        "<h2> ğŸˆ Cost function </h2>\n",
        "\n",
        "- Cost functionì€ loss functionì˜ í•©ì´ë‹¤. \n",
        "- ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë‚˜ìœì§€ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ <-> íš¨ìš©í•¨ìˆ˜\n",
        "- ì¦‰ entire data setì—ì„œ loss funcitonì„ ê³„ì‚°í•œ í•©ì´ë‹¤.\n",
        "- ìˆœê°„ìˆœê°„ì˜ lossë¥¼ íŒë‹¨í•  ë• loss function, í•™ìŠµì´ ì™„ë£Œëœ í›„ì—ëŠ” cost function!\n",
        "- A loss function is a part of a cost function which is a type of an objective function!!!\n",
        "\n",
        "<h2> ğŸˆ Entropy </h2>\n",
        "\n",
        "- ë¶ˆí™•ì‹¤ì„±ì— ëŒ€í•œ ì²™ë„.\n",
        "- Entropy í•¨ìˆ˜  \n",
        "  $H_p(X) = \\mathbb{E}\\big[I(X)\\big] = \\mathbb{E} \\big[ \\log (\\frac{1}{p(X)}) \\big] = -\\sum_{i=1}^{n} p(x_i)\\log(p(x_i))$  \n",
        "  CëŠ” ë²”ì£¼ì˜ ê°¯ìˆ˜, qëŠ” ì‚¬ê±´ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜(probability mass function)\n",
        "- ì˜ˆì¸¡ì´ ì–´ë ¤ìš¸ìˆ˜ë¡ ì •ë³´ì˜ ì–‘ì€ ë” ë§ì•„ì§€ê³  ì—”íŠ¸ë¡œí”¼ëŠ” ì»¤ì§„ë‹¤.\n",
        "- í™•ë¥ ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ **ì •ë³´ëŸ‰ì˜ í‰ê· **. ë†€ëŒì˜ ì •ë„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "<h2> ğŸˆ Estimator (ì¶”ì •ê¸°) </h2>\n",
        "\n",
        "- datasetì„ ê¸°ë°˜ìœ¼ë¡œ ì¼ë ¨ì˜ ëª¨ë¸ parameterë¥¼ ì¶”ì •í•˜ëŠ” ê°ì²´\n",
        "- ì¶”ì •ì€ fit( ) ë©”ì„œë“œì— ì˜í•´ ìˆ˜í–‰ë˜ê³  í•˜ë‚˜ì˜ ë§¤ê°œë³€ìˆ˜ë¡œ í•˜ë‚˜ì˜ datasetë§Œ ì „ë‹¬\n",
        "\n",
        "<h2> ğŸˆ Epoch </h2>\n",
        "\n",
        "- one epoch: ëª¨ë“  training exampleì„ ë„ëŠ” ê²ƒ!\n",
        "\n",
        "<h2> ğŸˆ Feature </h2>\n",
        "\n",
        "- attribute(ì†ì„±)ì™€ ê°’ì´ í•©ì³ì§„ í˜•íƒœ\n",
        "- e.g ì£¼í–‰ê±°ë¦¬ = 15,000\n",
        "\n",
        "<h2> ğŸˆ Gini Index </h2>\n",
        "\n",
        "- $G.I(A)=\\sum _{ i=1 }^{ d }{ { \\left( { R }_{ i }\\left( 1-\\sum _{ k=1 }^{ m }{ { p }_{ ik }^{ 2 } }  \\right)  \\right)  } }$\n",
        "- ì •ë³´ì˜ ìˆœë„ë¥¼ ì¸¡ì •í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì§€í‘œ ì¤‘ í•˜ë‚˜\n",
        "\n",
        "<h2> ğŸˆ Gradient </h2>\n",
        "\n",
        "- Parameterë“¤ì˜ í¸ë¯¸ë¶„ê³„ìˆ˜, ê¸°ìš¸ê¸°\n",
        "\n",
        "<h2> ğŸˆ Hold-out Validation </h2>\n",
        "\n",
        "- Validation setì„ ë§Œë“œëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜\n",
        "- ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ train setê³¼ validation set, test setìœ¼ë¡œ êµ¬ë¶„í•œ ë’¤, train setê³¼ validation setì„ ì´ìš©í•´ ë¶„ì„ ëª¨í˜•ì„ êµ¬ì¶•í•˜ê³ , test setì„ ì´ìš©í•˜ì—¬ ë¶„ì„ ëª¨í˜•ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²• \n",
        "- step.1\n",
        " - Original Setì„ ë¬´ì‘ìœ„ë¡œ train setê³¼ validation set, test setìœ¼ë¡œ êµ¬ë¶„!\n",
        "- step.2\n",
        "  - hyperparameterë¥¼ ë‹¤ë¥´ê²Œ í•´ ì—¬ëŸ¬ê°€ì§€ ì„¸íŒ…ì„ ë§Œë“¤ì–´ë³¸ ë’¤, í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ Train setì— ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë®¤\n",
        "- step.3\n",
        "  - Validation Setë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì˜ hyperparameter ì„¸íŒ…ì„ ì„ íƒí•¨\n",
        "- step.4\n",
        "  - Train Setì€ ë³´í†µ í´ìˆ˜ë¡ ì¢‹ìŒ. ê·¸ëŸ¬ë¯€ë¡œ ëª¨ë¸ ì„ íƒ í›„ì— Train Setì™€ Validation Setì„ í•©ì³ ë” í° ë°ì´í„°ì…‹ìœ¼ë¡œ step.3ì—ì„œ ì„ íƒí•œ ìµœì„ ì˜ hyperparameter ì„¸íŒ…ì„ ì‚¬ìš©í•œ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸°!\n",
        "- step.5 \n",
        "  - ì´ì œ Test Setì„ ì‚¬ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€~\n",
        "- Cross Validationê³¼ì˜ ì°¨ì´ì : Hold-out Validationì—ì„œ Test Setì˜ ê²½ìš° ëª¨í˜•ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•Šê³ , ëª¨ë¸ì˜ ì„±ëŠ¥ ì¸¡ì • ë§Œì„ ìœ„í•´ ì‚¬ìš©ëœë‹¤ëŠ” ì ! \n",
        "\n",
        "<h2> ğŸˆ Hyperparameter </h2>\n",
        "\n",
        "- í•™ìŠµì˜ ëŒ€ìƒì´ ì•„ë‹ˆë¼ í•™ìŠµ ì´ì „ì— ì •í•´ë†“ì€ ë³€ìˆ˜, í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ parameter\n",
        "- ParameterëŠ” í•™ìŠµì˜ ëŒ€ìƒì´ ë˜ëŠ” ë³€ìˆ˜\n",
        "\n",
        "<h2> ğŸˆ Iteration </h2>\n",
        "\n",
        "- batchë¥¼ ëª‡ ë²ˆ í•™ìŠµì— ì‚¬ìš©í–ˆëƒ?\n",
        "- e.g. 1000ê°œì˜ training setì´ ìˆê³ , batch sizeëŠ” 500ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ 1 epoch ë„ëŠ” ë™ì•ˆ 2 iterationì´ë‹¤.\n",
        "\n",
        "<h2> ğŸˆ Learning rate </h2>\n",
        "\n",
        "- ê³„ì‚°í•œ ê¸°ìš¸ê¸°ì— ë¹„ë¡€í•˜ì—¬ parameterë¥¼ ì–¼ë§ˆë§Œí¼ ì—…ë°ì´íŠ¸í• ì§€ ê²°ì •í•˜ëŠ” ìˆ˜ì¹˜\n",
        "- ë°°ìš°ëŠ” ì†ë„. ë°ì´í„°ì™€ ëª¨ë¸ì— ë”°ë¼ì„œ ìµœì ê°’ì€ ëª¨ë‘ ë‹¤ë¥´ë‹¤.\n",
        "- ë³´í†µ ì‹¤ë¬´ì—ì„œëŠ” ì´ˆê¸°ì— ë¹„êµì  ë†’ì€ lrë¡œ ì‹œì‘í•˜ì—¬ ì ì°¨ ë‚®ì¶”ëŠ” ì „ëµì„ ì·¨í•¨. ë‹¨ ì˜¤íˆë ¤ batch sizeë¥¼ ëŠ˜ë¦¬ëŠ” ê²Œ ë” ì¢‹ë‹¤ëŠ” ì—°êµ¬ë„ ìˆìŒ\n",
        "- Learning rateê°€ ë„ˆë¬´ í¬ë©´?? \n",
        "  - ë°œì‚°í•´ë²„ë¦¬ê²Œ ëœë‹¤. Costê°€ ë¬´ì§„ì¥ ëŠ˜ì–´ë‚œë‹¤.\n",
        "  - ë°ì´í„°ì— ë¹¨ë¦¬ ì ì‘í•˜ì§€ë§Œ ì´ì „ ë°ì´í„°ë¥¼ ê¸ˆë°© ê¹Œë¨¹ìŒ\n",
        "- Learning rateê°€ ë„ˆë¬´ ì‘ìœ¼ë©´? \n",
        "  - cost ê°’ì´ ë³€í•¨ì´ ì—†ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n",
        "  - ê´€ì„±ì´ ì»¤ì ¸ì„œ ëŠë¦¬ê²Œ í•™ìŠµë¨\n",
        "\n",
        "<h2> ğŸˆ Learning rate schedule </h2>\n",
        "\n",
        "- ë§¤ iterationì—ì„œ learning rateì„ ë°”ê¿”ì£¼ëŠ” í•¨ìˆ˜\n",
        "\n",
        "<h2> ğŸˆ Logit </h2>\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Logit.svg/350px-Logit.svg.png)\n",
        "- [Logit, Sigmoid, Softmaxì˜ ê´€ê³„](https://opentutorials.org/module/3653/22995)\n",
        "\n",
        "<h2> ğŸˆ Loss function   </h2>\n",
        "\n",
        "- Loss functionì€ data pointì—ì„œì˜ ì˜¤ì°¨. Single data setì—ì„œì˜ ì˜¤ì°¨ë¥¼ êµ¬í•˜ëŠ” ê²ƒ\n",
        "ìˆœê°„ìˆœê°„ì˜ lossë¥¼ íŒë‹¨í•  ë• loss function, í•™ìŠµì´ ì™„ë£Œëœ í›„ì—ëŠ” cost function!\n",
        "- A loss function is a part of a cost function which is a type of an objective function!!!\n",
        "\n",
        "<h2> ğŸˆ MAE (Mean Absolute Error) </h2>\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_predict)\n",
        "```\n",
        "\n",
        "<h2> ğŸˆ MAPE (Mean Asolute Percentage Error) </h2>\n",
        "\n",
        "- Scale Dependent Errorì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ë‹¤.\n",
        "- í•˜ì§€ë§Œ MAPE ì—­ì‹œ ì‹¤ì œ ì˜ˆì¸¡ ê°’ì´ 1ë³´ë‹¤ ì‘ì„ ê²½ìš° ë¶„ëª¨ê°€ ì‘ì•„ì ¸ ë¬´í•œëŒ€ì— ê°€ê¹Œì›Œì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.\n",
        "``` python\n",
        "from sklearn.utils import check_arrays\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "    y_true, y_pred = check_arrays(y_true, y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "```\n",
        "\n",
        "<h2> ğŸˆ MSE(Mean Squared Error) </h2>\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(y_test, y_predict)\n",
        "```\n",
        "\n",
        "<h2> ğŸˆ Objective function </h2>\n",
        "\n",
        "- The most general term for any function that you optimize during training.\n",
        "- For example, a probability of generating training set in maximum likelihood approach is a well defined objective function, but it is not a loss function nor cost function (however you could define an equivalent cost function). For example:\n",
        "\n",
        "<h2> ğŸˆ Online learning <-> Batch learning </h2>\n",
        "\n",
        "- ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•œ ê°œì”© ë˜ëŠ” mini-batch ë‹¨ìœ„ë¡œ ì£¼ì…í•´ì„œ ì‹œìŠ¤í…œì„ í›ˆë ¨ì‹œí‚´\n",
        "- ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë„ì°©í•˜ëŠ” ëŒ€ë¡œ ì¦‰ì‹œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "\n",
        "<h2> ğŸˆ Overfitting </h2>\n",
        "\n",
        " - ëª¨ë¸ì´ Train Datasetì— ë„ˆë¬´ ì˜ ë§ì•„ì„œ ì¼ë°˜ì„±ì´ ë–¨ì–´ì§€ê²Œ ë˜ëŠ” ë¬¸ì œ\n",
        " - ì¦‰ Train Datasetì„ ë„ˆë¬´ ê³¼í•˜ê²Œ í•™ìŠµí•´ í•™ìŠµë˜ì§€ ì•Šì€ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ë¶„ë¥˜í•˜ì§€ ëª»í•˜ê²Œ ë˜ëŠ” ê²ƒ\n",
        " - ìœ„ ê·¸ë¦¼ì˜ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ë“¤ì´ ê³¼ëŒ€ì í•©ì˜ ì˜ˆì‹œ\n",
        " - ë³´ì‹œë‹¤ì‹œí”¼ Train Setì„ ê±°ì˜ ë‹¤ ê±°ì¹˜ê±°ë‚˜ ë¶„ë¥˜í•´ë‚´ë©° êµ‰ì¥íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆì§€ë§Œ, ìƒˆë¡œìš´ ë³€ìˆ˜ì— ëŒ€ì‘í•˜ê¸° ì–´ë ¤ì›€\n",
        " - ê·¸ë¦¼ì˜ ê°€ìš´ë° ëª¨ë¸ì´ ì í•©í•œ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆìŒ!\n",
        "\n",
        "<h2> ğŸˆ OvA ì „ëµ </h2>\n",
        "\n",
        "- label ê°œìˆ˜ë§Œí¼ ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“œëŠ” ë°©ë²•\n",
        "\n",
        "<h2> ğŸˆ OvO ì „ëµ </h2>\n",
        "\n",
        "- label combination 2ë§Œí¼ì˜ ì¡°í•©ë§ˆë‹¤ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë¶„ë¥˜ê¸°ë¥¼ ë‹¤ ë§Œë“œëŠ” ë°©ë²•\n",
        "\n",
        "<h2> ğŸˆ Partial Derivative </h2>\n",
        "\n",
        "- í¸ë„í•¨ìˆ˜\n",
        "- ëª¨ë¸ parameterì— ëŒ€í•´ ë¹„ìš© í•¨ìˆ˜ gradientë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒ!\n",
        "- ì¦‰, parameterê°€ ì¡°ê¸ˆ ë³€ê²½ë  ë•Œ cost functionì´ ì–¼ë§ˆë‚˜ ë°”ë€ì§€ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
        "\n",
        "<h2> ğŸˆ Precision (ì •ë°€ë„) </h2>\n",
        "\n",
        "- ì •ë°€ë„(precision)ë€ positive í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ë¶„ë¥˜í•œ ìƒ˜í”Œ ì¤‘ ì‹¤ì œë¡œ positive í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ìƒ˜í”Œ ìˆ˜ì˜ ë¹„ìœ¨\n",
        "- ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨í˜•!!!\n",
        "$$precision=\\frac{TP}{TP+FP}$$\n",
        "``` python\n",
        "from sklearn.metrics import precision_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "precision_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "<h2> ğŸˆ Recall (ì¬í˜„ìœ¨) </h2>\n",
        "\n",
        "- ì¬í˜„ìœ¨(recall)ì´ë€ ì‹¤ì œ positive í´ë˜ìŠ¤ì— ì†í•œ í‘œë³¸ ì¤‘ì— positive í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ì¶œë ¥í•œ í‘œë³¸ì˜ ìˆ˜ì˜ ë¹„ìœ¨\n",
        "- ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨í˜•!!\n",
        "- TPR(true positive rate) ë˜ëŠ” ë¯¼ê°ë„(sensitivity)ë¼ê³ ë„ í•©ë‹ˆë‹¤.\n",
        "$$recall=\\frac{TP}{TP+FN}$$\n",
        "``` python\n",
        "from sklearn.metrics import recall_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "recall_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "<h2> ğŸˆ ROC </h2>\n",
        "\n",
        "- ROCì´ë€ TPRê³¼ FPRì€ ì–´ë–¤ ê¸°ì¤€ì„ ì—°ì†ì ìœ¼ë¡œ ë°”ê¾¸ë©° ì¸¡ì •í•´ì•¼ í•˜ëŠ”ë° ì´ë¥¼ í•œëˆˆì— ë³¼ ìˆ˜ ìˆê²Œ í•œ ê²ƒ\n",
        "- ![ROC_curve](https://t1.daumcdn.net/cfile/tistory/262E8E3F544837AD27)<br>\n",
        "- ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ROC ê³¡ì„ ì€ TPRê³¼ FPRì´ ë‘˜ë‹¤ [0,1]ì˜ ë²”ìœ„ì´ë©° (0,0)ì—ì„œ (1,1)ì„ ì‡ëŠ” ê³¡ì„ ì´ë‹¤. \n",
        "\n",
        "<h2> ğŸˆ Sampling Noise </h2>\n",
        "\n",
        "- ìƒ˜í”Œì´ ì‘ì„ ë•Œ ìš°ì—°ì— ì˜í•œ ëŒ€í‘œì„±ì´ ì—†ëŠ” ë°ì´í„°ê°€ ìƒê²¼ìŒì„ ì˜ë¯¸\n",
        "\n",
        "<h2> ğŸˆ Sigmoid function </h2>\n",
        "\n",
        "- $S(x) = \\frac{1}{1 + e^{-x}}$\n",
        "\n",
        "<h2> ğŸˆ Transformer(ë³€í™˜ê¸°) </h2>\n",
        "\n",
        "- datasetì„ ë³€í™˜í•˜ëŠ” estimatorë¥¼ transformerë¼ê³  í•¨\n",
        "\n",
        "<h2> ğŸˆ Underfitting </h2>\n",
        "\n",
        " - Overfittingê³¼ ë°˜ëŒ€ ê°œë…\n",
        " - ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ ë°ì´í„°ì˜ ë‚´ì¬ëœ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ê²ƒ\n",
        " - ìœ„ ê·¸ë¦¼ì˜ ì™¼ìª½ ê·¸ë¦¼ì´ underfitting.\n",
        "\n",
        "<h2> ğŸˆ Variance </h2>\n",
        "\n",
        " - VarianceëŠ” ì˜ˆì¸¡ëœ ê°’ë“¤ì´ ì„œë¡œ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ì— ëŒ€í•œ ì²™ë„"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-zlueslZ7fe"
      },
      "source": [
        "# âœï¸ Types of Machine Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IknJz98B6eFD"
      },
      "source": [
        "![123123](https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/styles/simple_image/public/images/machine-learning1.png?SnePeroHk5B9yZaLY7peFkULrfW8Gtaf&itok=yjEJbEKD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0goZ-1yFjQ6l"
      },
      "source": [
        "- Machine Learningì—ëŠ” í¬ê²Œ ì§€ë„í•™ìŠµ(Supervised Learning), ë¹„ì§€ë„í•™ìŠµ(Unsupervised Learning)ìœ¼ë¡œ ë‚˜ë‰œë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ7DZyJVcHzi"
      },
      "source": [
        "\n",
        "> ì§€ë„í•™ìŠµ\n",
        "- ë‹µì´ ì´ë¯¸ ìˆëŠ” ë°ì´í„°ë¥¼ ê¸°ê³„ê°€ í•™ìŠµí•œ ë’¤, ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ ë‹µì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•\n",
        "- Regression, Classificationì´ ì´ì— í•´ë‹¹\n",
        "\n",
        "> ë¹„ì§€ë„í•™ìŠµ\n",
        " - ë‹µì´ ì—†ëŠ” ë°ì´í„°ì˜ íŠ¹ì§•ë“¤ì„ ê¸°ê³„ê°€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•œ ë’¤ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µì„ ë‚´ì£¼ëŠ” ë°©ë²•\n",
        " - Clustering, Dimensionality Reduction, Visualization, ì—°ê´€ê·œì¹™í•™ìŠµì´ ì´ì— í•´ë‹¹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKvXhJJYZ7nq"
      },
      "source": [
        "# âœï¸ Overall Process of Machine Learning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSRYORPDZ7uv"
      },
      "source": [
        "## ğŸ” 1. Set Project's Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5sW7b_7ah0T"
      },
      "source": [
        "- ìš°ë¦¬ê°€ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œê°€ ë­”ì§€?\n",
        "- Machine Learning ëª¨ë¸ì„ í†µí•´ì„œ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œë¥¼ ì„¤ì •!\n",
        "\n",
        "> ì˜ˆì‹œ\n",
        " - ë§¤ì¶œ íšŒë³µì„ ìœ„í•´ ê³ ê°ë³„ STP ì „ëµì„ í™œìš©í•œ íƒ€ê²Ÿ ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½\n",
        " - ì‚¬ê¸°ê±°ë˜ì˜ ë¹„ì¤‘ì´ ë†’ì•„ì§€ëŠ”ë° ì‚¬ê¸°ê±°ë˜ë¥¼ ì˜ˆì¸¡ì„ í†µí•œ ë¹„ìš© ê°ì†Œ\n",
        " - ê³ ê°ë“¤ì´ ì£¼ë¬¸í•  ìƒí’ˆëŸ‰ì„ ì˜ˆì¸¡ì„ í†µí•œ ë¬¼ë¥˜ ì‹œìŠ¤í…œ íš¨ìœ¨ì„± ì¦ì§„\n",
        " - ì–´ë¦°ì•„ì´ì—ê²Œ ì•ˆì „í•œ ë™ì˜ìƒì„ ê±¸ëŸ¬ë‚´ì£¼ëŠ” ëª¨ë¸ ë§Œë“¤ê¸°\n",
        " - CCTVë¡œ ì¢€ë„ë‘‘ì„ ì¡ëŠ” ëª¨ë¸ ë§Œë“¤ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aGwgB_wbCrO"
      },
      "source": [
        "## ğŸ” 2. Select Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s61epZtz_JRI"
      },
      "source": [
        "### ğŸˆ Regression Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khIL1lzv_JRJ"
      },
      "source": [
        "- Regressionì€ í‰ê°€í•  ë•Œ **ì‹¤ì œ ë‹µê³¼ ì˜ˆì¸¡ê°’**ì´ ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ìˆëŠ”ì§€ë¡œ í‰ê°€!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TEuHGJ5_JRJ"
      },
      "source": [
        "- MAE, MSE, RMSE ëª¨ë‘ vectorì™€ target vector ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¬ëŠ” ë°©ë²•\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la4LoIqkmnNm"
      },
      "source": [
        "> ğŸ“ MAE(Mean Absolute Error)\n",
        "- $MAE = \\frac{1}{N}\\sum_{(i=1)}^N\\left\\lvert{f_i-y_i}\\right\\rvert$\n",
        " ```python\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_predict)\n",
        "```\n",
        "\n",
        "> ğŸ“ MSE(Mean Squared Error)\n",
        "- $MSE = \\frac{1}{N}\\sum_{(i=1)}^N(f_i-y_i)^2$\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(y_test, y_predict)\n",
        "```\n",
        "\n",
        "> ğŸ“ RMSE(Root Mean Squared Error)  \n",
        "- $RMSE = \\sqrt{\\frac{1}{N}\\sum_{(i=1)}^N(f_i-y_i)^2}$\n",
        "- RMSEê°€ MAEë³´ë‹¤ ì¡°ê¸ˆ ë” outlierì— ë¯¼ê°í•¨\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse\n",
        "```\n",
        "\n",
        "> ğŸ“ MAPE  \n",
        "- $MAPE = \\frac{1}{n}\\sum_{t=1}^n |\\frac{y_i-f_i}{y_i}|$\n",
        "- Scale Dependent Errorì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ë‹¤.\n",
        "- í•˜ì§€ë§Œ MAPE ì—­ì‹œ ì‹¤ì œ ì˜ˆì¸¡ ê°’ì´ 1ë³´ë‹¤ ì‘ì„ ê²½ìš° ë¶„ëª¨ê°€ ì‘ì•„ì ¸ ë¬´í•œëŒ€ì— ê°€ê¹Œì›Œì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.\n",
        "```python\n",
        "from sklearn.utils import check_arrays\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "    y_true, y_pred = check_arrays(y_true, y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11nm8K92_JRV"
      },
      "source": [
        "### ğŸˆ Classification Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7zE1spi_JRW"
      },
      "source": [
        "<img src=\"https://t1.daumcdn.net/cfile/tistory/99DC064C5BE056CE10\" alt=\"Drawing\" style=\"width: 100px;\"/>  \n",
        "- True Positive(TP) : ì‹¤ì œ Trueì¸ ì •ë‹µì„ Trueë¼ê³  ì˜ˆì¸¡ (ì •ë‹µ)\n",
        "- False Positive(FP) : ì‹¤ì œ Falseì¸ ì •ë‹µì„ Trueë¼ê³  ì˜ˆì¸¡ (ì˜¤ë‹µ)\n",
        "- False Negative(FN) : ì‹¤ì œ Trueì¸ ì •ë‹µì„ Falseë¼ê³  ì˜ˆì¸¡ (ì˜¤ë‹µ)\n",
        "- True Negative(TN) : ì‹¤ì œ Falseì¸ ì •ë‹µì„ Falseë¼ê³  ì˜ˆì¸¡ (ì •ë‹µ)<br>\n",
        "[reference](https://frhyme.github.io/machine-learning/clf_%ED%8F%89%EA%B0%80%ED%95%98%EA%B8%B0/)  \n",
        "\n",
        "\n",
        "> <h3> ğŸ“ ì •í™•ë„(accuracy) </h3>\n",
        "\n",
        "- ì •í™•ë„(accuracy)ë€ ì „ì²´ ìƒ˜í”Œ ì¤‘ ë§ê²Œ ì˜ˆì¸¡í•œ ìƒ˜í”Œ ìˆ˜ì˜ ë¹„ìœ¨\n",
        "- ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨í˜•ì´ê³  ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµì—ì„œ ìµœì í™” ëª©ì í•¨ìˆ˜ë¡œ ì‚¬ìš©ë¨!!\n",
        "- ì •í™•ë„(Accuracy)ì˜ ê°€ì¥ í° ë¬¸ì œì ì€ **í´ë˜ìŠ¤ì˜ ë¶„í¬ê°€ ê°™ì„ ë•Œë§Œ ì´ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ì **\n",
        "- ì´ëŸ¬í•œ **ë‹¨ì ì„ ë³´ì™„**í•˜ëŠ” ì§€í‘œê°€ ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall), ROC ê³¡ì„ ê³¼ AUCì…ë‹ˆë‹¤.\n",
        "$$accuracy=\\frac{TP+TN}{TP+TN+FP+FN}$$<br>\n",
        "``` python\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "accuracy_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "> <h3> ğŸ“ ì •ë°€ë„(precision) </h3>\n",
        "\n",
        "- ì •ë°€ë„(precision)ë€ positive í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ë¶„ë¥˜í•œ ìƒ˜í”Œ ì¤‘ ì‹¤ì œë¡œ positive í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ìƒ˜í”Œ ìˆ˜ì˜ ë¹„ìœ¨\n",
        "- ë†’ì´ë ¤ë©´ ì§„ì§œì§„ì§œ ì•„ì£¼ì•„ì£¼ í™•ì‹¤í•œ ê²ƒ í•˜ë‚˜ë§Œ ì œëŒ€ë¡œ ì˜ˆì¸¡í•˜ë©´ ì™„ë²½í•œ ì •ë°€ë„ë¥¼ ì–»ìŒ!\n",
        "- ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨í˜•!!!\n",
        "$$precision=\\frac{TP}{TP+FP}$$\n",
        "``` python\n",
        "from sklearn.metrics import precision_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "precision_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "> <h3> ğŸ“ ì¬í˜„ìœ¨(recall) </h3>\n",
        "\n",
        "- ì¬í˜„ìœ¨(recall)ì´ë€ ì‹¤ì œ positive í´ë˜ìŠ¤ì— ì†í•œ í‘œë³¸ ì¤‘ì— positive í´ë˜ìŠ¤ì— ì†í•œë‹¤ê³  ì¶œë ¥í•œ í‘œë³¸ì˜ ìˆ˜ì˜ ë¹„ìœ¨\n",
        "- ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨í˜•!!\n",
        "- TPR(true positive rate) ë˜ëŠ” ë¯¼ê°ë„(sensitivity)ë¼ê³ ë„ í•©ë‹ˆë‹¤.\n",
        "$$recall=\\frac{TP}{TP+FN}$$\n",
        "``` python\n",
        "from sklearn.metrics import recall_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "recall_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "> <h3> ğŸ“ F Score, F1 score </h3>\n",
        "\n",
        "- F ScoreëŠ” precisionê³¼ recallì˜ **ê°€ì¤‘ì¡°í™”í‰ê· **\n",
        "- ë² íƒ€($\\beta$)ëŠ” ì •ë°€ë„ì— ì£¼ì–´ì§€ëŠ” ê°€ì¤‘ì¹˜\n",
        "$$F_Î²=\\frac{(1+Î²^2)(precision*recall)}{Î²^2precision+recall}$$  \n",
        "- ë² íƒ€ê°€ 1ì¸ ê²½ìš°ê°€ F1 Score!\n",
        "$$F_1=\\frac{2â‹…precisionâ‹…recall}{precision+recall}$$\n",
        "``` python\n",
        "from sklearn.metrics import f1_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "f1_score(y_true, y_pred)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFKTUvk_L7wT"
      },
      "source": [
        "> <h3> ğŸ“ Precision vs Recall vs F1 score </h3>\n",
        "\n",
        "- ë¬´ì¡°ê±´ F1 scoreë§Œ ë³´ëŠ” ê²ƒì€ ì˜ëª» ë¨!\n",
        "- ë§¨ ì²˜ìŒ í° ê·¸ë¦¼ì„ ë³¼ ë•Œ ëª©í‘œ/ë¬¸ì œ ì •ì˜ì— ë”°ë¼ ë´ì•¼ë˜ëŠ” ì„±ëŠ¥ì§€í‘œê°€ ë‹¬ë¼ì§„ë‹¤!\n",
        "- ìœ í•´ë™ì˜ìƒì„ ì–´ë¦°ì•„ì´ì—ê²Œ ê±¸ëŸ¬ë‚´ëŠ” ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œí‚¨ë‹¤ë©´ ì¢‹ì€ ë™ì˜ìƒì´ ë§ì´ ì œì™¸ë˜ë”ë¼ë„(ë‚®ì€ ì¬í˜„ìœ¨) ì•ˆì „í•œ ê²ƒë“¤ë§Œ ë…¸ì¶œì‹œí‚¤ëŠ”(ë†’ì€ ì •ë°€ë„) ëª¨ë¸ì´ ì¢‹ìŒ!\n",
        "- ì¢€ë„ë‘‘ì„ ì¡ëŠ” ë¶„ë¥˜ê¸°ë¼ë©´ ê²½ë¹„ì›ì´ ì˜ëª»ëœ í˜¸ì¶œë¡œ ê³ ìƒí•˜ì…”ë„(ì •í™•ë„ê°€ 30%ë¼ì„œ) ì¬í˜„ìœ¨ì´ 99%ë¼ë©´ ëª¨ë“  ì¢€ë„ë‘‘ì„ ì¡ì€ ê²ƒì´ê¸° ë•Œë¬¸ì— ì¢‹ì€ ëª¨ë¸! \n",
        "- ë¶„ë¥˜ê¸°ì—ì„œ predict( ) ë©”ì„œë“œ ëŒ€ì‹  decision_function( ) ë©”ì„œë“œë¥¼ ì“°ë©´ [ìƒ˜í”Œì˜ ì ìˆ˜]ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
        "- ì´ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ thresholdë¥¼ ì •í•´ ì˜ˆì¸¡ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.\n",
        "- precision_recall_cureve( ) í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ ëª¨ë“  thresholdì— ëŒ€í•´ recallê³¼ precisionì„ ê³„ì‚°ê°€ëŠ¥\n",
        "```python\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
        "```\n",
        "- ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
        "```python\n",
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"ì •ë°€ë„\", linewidth=2)\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"ì¬í˜„ìœ¨\", linewidth=2)\n",
        "    plt.xlabel(\"ì„ê³„ê°’\", fontsize=16)\n",
        "    plt.legend(loc=\"upper left\", fontsize=16)\n",
        "    plt.ylim([0, 1])\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "plt.xlim([-700000, 700000])\n",
        "save_fig(\"precision_recall_vs_threshold_plot\")\n",
        "plt.show()\n",
        "```\n",
        "![](data:image/png;base64,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)\n",
        "\n",
        "- ì‘ì—…ì— ë§ëŠ” ìµœì„ ì˜ thresholdë¥¼ ì„ íƒí•˜ë©´ ë¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-XhT0BCL-WQ"
      },
      "source": [
        "> <h3> ğŸ“ Classification_report </h3>\n",
        "\n",
        "- Classification reportëŠ” sklearn íŒ¨í‚¤ì§€ì˜ metrics íŒ¨í‚¤ì§€ì—ì„œ precision, recall , F1 scoreë¥¼ êµ¬í•  ë•Œ ì“°ëŠ” ë©”ì†Œë“œ\n",
        "- Classification reportëŠ” ê°ê°ì˜ í´ë˜ìŠ¤ë¥¼ ì–‘ì„±(positive) í´ë˜ìŠ¤ë¡œ ë³´ì•˜ì„ ë•Œì˜  precision, recall , F1 scoreë¥¼ ê°ê° êµ¬í•˜ê³  ê·¸ í‰ê· ê°’ìœ¼ë¡œ ì „ì²´ ëª¨í˜•ì˜ ì„±ëŠ¥ì„ í‰ê°€í•¨!!\n",
        "\n",
        "``` python\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=['class 0', 'class 1']))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_doQAvvMAh4"
      },
      "source": [
        "> <h3> ğŸ“ ROC, AUC </h3>\n",
        "\n",
        "- ROCì´ë€ TPRê³¼ FPRì€ ì–´ë–¤ ê¸°ì¤€ì„ ì—°ì†ì ìœ¼ë¡œ ë°”ê¾¸ë©° ì¸¡ì •í•´ì•¼ í•˜ëŠ”ë° ì´ë¥¼ í•œëˆˆì— ë³¼ ìˆ˜ ìˆê²Œ í•œ ê²ƒ\n",
        "- AUCë€ ROC ê³¡ì„  ì•„ë˜ ë¶€ë¶„ì˜ ë©´ì ì´ë‹¤. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ìŒ!\n",
        "- ![ROC_curve](https://t1.daumcdn.net/cfile/tistory/262E8E3F544837AD27)<br>\n",
        "- ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ROC ê³¡ì„ ì€ TPRê³¼ FPRì´ ë‘˜ë‹¤ [0,1]ì˜ ë²”ìœ„ì´ë©° (0,0)ì—ì„œ (1,1)ì„ ì‡ëŠ” ê³¡ì„ ì´ë‹¤. \n",
        "\n",
        "- TRP(True Positive Rate) = recall\n",
        "$$TPR=\\frac{TP}{TP+FN}$$<br>\n",
        "- FPR(False Positive Rate)\n",
        "$$FPR=\\frac{FP}{FP+TN}$$<br>\n",
        "- TPRê³¼ FPRì€ ë°˜ë¹„ë¡€ ê´€ê³„ì— ìˆìŒ\n",
        "  - ì˜ˆì‹œ) 3ì¸ì§€ ì•„ë‹Œì§€ íŒë‹¨í•  ë•Œ, ì¡°ê¸ˆë§Œ 3ì²˜ëŸ¼ ë³´ì—¬ë„ ëª¨ë‘ 3ì´ë¼ê³  ë¶„ë¥˜í•  ê²½ìš°, ì´ ë•Œì˜ TPRì€ 1ì— ê°€ê¹Œì›Œì§€ì§€ë§Œ ë°˜ëŒ€ë¡œ FPRì€ ë§¤ìš° ë‚®ì•„ì§„ë‹¤. ë°˜ëŒ€ë¡œ ì¡°ê¸ˆë§Œ ë¹„ìŠ·í•˜ì§€ ì•Šì•„ë„ ëª¨ë‘ 3ì´ ì•„ë‹ˆë¼ê³  ë¶„ë¥˜í•  ê²½ìš° TPRì€ ê¸‰ê²©íˆ ë‚®ì•„ì ¸ 0ì— ê°€ê¹Œì›Œì§€ê² ì§€ë§Œ, FPRì€ ë°˜ëŒ€ë¡œ ê¸‰ê²©íˆ ë†’ì•„ì ¸ 1ì— ê°€ê¹Œì›Œì§„ë‹¤.(3ì´ë¼ê³  ë¶„ë¥˜ ìì²´ë¥¼ ì•ˆí•˜ë¯€ë¡œ, ì˜ëª» ë¶„ë¥˜í•˜ëŠ” ê²½ìš°ê°€ ì—†ëŠ” ê²ƒ). ì´ì²˜ëŸ¼ TPRê³¼ FPRì€ ì–´ë–¤ ê¸°ì¤€(ì–¸ì œ 3ì´ë¼ê³  ì˜ˆì¸¡í• ê¹Œ?)ì„ ì—°ì†ì ìœ¼ë¡œ ë°”ê¾¸ë©° ì¸¡ì •í•´ì•¼ í•˜ëŠ”ë° ì´ë¥¼ í•œëˆˆì— ë³¼ ìˆ˜ ìˆê²Œ í•œ ê²ƒì´ ë°”ë¡œ **ROC ê³¡ì„ **ì…ë‹ˆë‹¤.  \n",
        "\n",
        "```python\n",
        "from sklearn.metrics import roc_auc_score\n",
        "y_true = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
        "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
        "roc_auc_score(y_true, y_scores)\n",
        "```\n",
        "\n",
        "- ì‹œê°í™”\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.metrics import accuracy_score, precision_score, precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def learn_and_eval_clf(x, y_true):\n",
        "    clf = LogisticRegression(random_state=42)\n",
        "    clf.fit(x, y_true)\n",
        "    y_pred = clf.predict(x)\n",
        "    print(\"accuracy_score: {}\".format( accuracy_score(y_true, y_pred)))\n",
        "    print(\"precision_score: {}\".format( precision_score(y_true, y_pred)))\n",
        "\n",
        "    y_score = clf.decision_function(x)\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "\n",
        "    f, axes = plt.subplots(1, 2, sharex=True, sharey=True)\n",
        "    f.set_size_inches((8, 4)) \n",
        "    axes[0].fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
        "    axes[0].set_title('Recall-Precision Curve')\n",
        "\n",
        "    axes[1].plot(fpr, tpr)\n",
        "    axes[1].plot([0, 1], [0, 1], linestyle='--')\n",
        "    axes[1].set_title('ROC curve')\n",
        "    #plt.save\n",
        "    return f\n",
        "\n",
        "## - ì„ì˜ë¡œ ê°ê° í‰ê· ì´ 0.0, 0.25ì¸, í° ì°¨ì´ê°€ ë‚˜ì§€ ì•ŠëŠ” ìƒ˜í”Œë“¤ì„ ë½‘ì•„ì„œ, ë¶„ë¥˜í•´ë³¸ë‹¤. \n",
        "\n",
        "sample_size = 100\n",
        "x = np.vstack(\n",
        "    [np.random.normal(0, 1, sample_size*2).reshape(sample_size, 2), \n",
        "     np.random.normal(0.25, 1, sample_size*2).reshape(sample_size, 2), \n",
        "    ]\n",
        ")\n",
        "y_true = np.array([0 for i in range(0, sample_size)]+[1 for i in range(0, sample_size)])\n",
        "\n",
        "\n",
        "try1 = learn_and_eval_clf(x, y_true)\n",
        "\n",
        "\n",
        "### ì„ì˜ë¡œ ê°ê° í‰ê· ì´ 0.0, 2ì¸, í° ì°¨ì´ê°€ ë‚˜ì§€ ì•ŠëŠ” ìƒ˜í”Œë“¤ì„ ë½‘ì•„ì„œ, ë¶„ë¥˜í•´ë³¸ë‹¤. \n",
        "\n",
        "sample_size = 100\n",
        "x = np.vstack(\n",
        "    [np.random.normal(0, 1, sample_size*2).reshape(sample_size, 2), \n",
        "     np.random.normal(2, 1, sample_size*2).reshape(sample_size, 2), \n",
        "    ]\n",
        ")\n",
        "y_true = np.array([0 for i in range(0, sample_size)]+[1 for i in range(0, sample_size)])\n",
        "try2 = learn_and_eval_clf(x, y_true)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShwcwsGVMHrp"
      },
      "source": [
        "> <h3> ğŸ“ ì–¸ì œ ROCë¥¼ ì“°ê³  ì–¸ì œ PR ê³¡ì„ ì„ ì“¸ê¹Œ? </h3>\n",
        "\n",
        "- PRê³¡ì„ : ì¼ë°˜ì ìœ¼ë¡œëŠ” ì–‘ì„± í´ë˜ìŠ¤ê°€ ë“œë¬¼ê±°ë‚˜ FNë³´ë‹¤ FPê°€ ë” ì¤‘ìš”í•  ë•Œ\n",
        "  - e.g 5ì™€ 5ê°€ ì•„ë‹Œ ìˆ«ì êµ¬ë¶„ ë¶„ë¥˜ê¸°\n",
        "- ROCê³¡ì„ : ë‚˜ë¨¸ì§€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdpFMZREL4_F"
      },
      "source": [
        "> <h3> ğŸ“ Confusion Matrix </h3>\n",
        "\n",
        "- ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ì—ëŸ¬ì˜ ì¢…ë¥˜ë¥¼ ë¶„ì„í•˜ëŠ” ë°©ë²•!\n",
        "- ê°€ìš´ë° ëŒ€ê°ì„ ì´ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ëœ ê²ƒì´ê³  ë‚˜ë¨¸ì§€ê°€ ì˜ëª» ë¶„ë¥˜ëœ ê²ƒë“¤ì´ë‹¤!\n",
        "- ì˜¤ì°¨í–‰ë ¬ì„ ë¶„ì„í•˜ë©´ classifierì˜ ì„±ëŠ¥ í–¥ìƒ ë°©ì•ˆì— ëŒ€í•œ í†µì°°ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "![](https://www.harrisgeospatial.com/docs/html/images/Classification/ConfusionMatrix.gif)\n",
        "\n",
        "```python\n",
        "# confusion matrix ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœ\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_train_5, y_train_pred)\n",
        "# confusion matrix ê·¸ë ¤ë³´ê¸°\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
        "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
        "conf_mx\n",
        "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
        "# color confusion matrix ê·¸ë¦¬ëŠ” ë°©ë²•\n",
        "def plot_confusion_matrix(matrix):\n",
        "    \"\"\"ì»¬ëŸ¬ ì˜¤ì°¨ í–‰ë ¬ì„ ì›í•  ê²½ìš°\"\"\"\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(matrix)\n",
        "    fig.colorbar(cax)\n",
        "# ì—ëŸ¬ ë¹„ìœ¨ì„ ë¹„êµí•˜ëŠ” ë²•! -> ê°œìˆ˜ë¡œ ë¹„êµí•˜ë©´ ì´ë¯¸ì§€ê°€ ë§ì€ í´ë˜ìŠ¤ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚˜ì˜ê²Œ ë³´ì„\n",
        "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
        "norm_conf_mx = conf_mx / row_sums\n",
        "\n",
        "np.fill_diagonal(norm_conf_mx, 0)\n",
        "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
        "save_fig(\"confusion_matrix_errors_plot\", tight_layout=False)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM_DC19NbFQ9"
      },
      "source": [
        "## ğŸ” 3. Set Environment and Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO4CTWK6bFWC"
      },
      "source": [
        "<h3> ğŸˆ 1) Set Environment </h3>\n",
        "\n",
        "> Step.1 pip ì„¤ì¹˜í•˜ê¸°\n",
        "- pythonì´ ì €ì¥ëœ ê²½ë¡œ\\Scriptsì— pip.exeì´ ìˆë‹¤.\n",
        "- ì´ pipì„ ê°€ì ¸ì™€ì„œ pipì„ ì„¤ì¹˜í•˜ì!\n",
        "```terminal\n",
        "$ pip3 install --upgrade pip\n",
        "```\n",
        "ìœ¼ë¡œ ìµœì‹ ë²„ì „ pip ì—…ë°ì´íŠ¸!  \n",
        "\n",
        "> tep.2 í•„ìš”í•œ íŒ¨í‚¤ì§€ ê¹”ê¸°\n",
        "- \n",
        "```terminal\n",
        "$ pip3 install --upgrade jupyter matplotlib numpy pandas scipy scikit-learn\n",
        "```\n",
        "- \n",
        "```terminal\n",
        "$ python3 -c \"import jupyter matplotlib numpy pandas scipy scikit-learn\"\n",
        "```\n",
        "- [Mac ìœ ì € ê°œë°œí™˜ê²½ êµ¬ì¶•](https://subicura.com/2017/11/22/mac-os-development-environment-setup.html)  \n",
        "ì´ ëª…ë ¹ì„ ì‹¤í–‰í–ˆì„ ë•Œ ì–´ë–¤ ì—ëŸ¬, ë©”ì„¸ì§€ë„ ì¶œë ¥ë˜ì§€ ì•Šìœ¼ë©´ ì„±ê³µ!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iywps9zRbGlz"
      },
      "source": [
        "<h3> ğŸˆ 2) Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNqo__Q-pksC"
      },
      "source": [
        "\n",
        "- External data, Internal data, Primary data, Secondary data\n",
        " - Internal data: Base Data(Application, warranty card), Channel data\n",
        " - External data: Survey, Consulting data, Alliance Data\n",
        " - Primary data: í˜„ì¡´í•˜ì§€ ì•Šê³  ë§Œë“œëŠ” ë°ì´í„°\n",
        " - Secondary data: ì´ë¯¸ ë‚´ê°€ ê°€ì§€ê³  ìˆê±°ë‚˜ ë‚¨ì´ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°\n",
        "- í¬ë¡¤ë§, ê³µê³µí¬í„¸ ë“±ì—ì„œ ê°€ì ¸ì™€ë„ ë˜êµ¬~\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdTwmwdvbGua"
      },
      "source": [
        "<h3> ğŸˆ Split Data (Train, Test, Validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6pj3xjW_JRq"
      },
      "source": [
        "![datasets](https://t1.daumcdn.net/cfile/tistory/9951E5445AAE1BE025)\n",
        "\n",
        "> train_test_split \n",
        "- test_sizeë¥¼ ì§€ì •í•´ì£¼ê¸°ë§Œ í•˜ë©´ ì•Œì•„ì„œ trainê³¼ testë¥¼ splití•¨!\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "```  \n",
        "- numpyë¡œ train, test ì°¢ê¸°\n",
        "```python\n",
        "import numpy as np\n",
        "shuffle_index = np.random.permutation(60000)\n",
        "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
        "```\n",
        "\n",
        "> StratifiedShuffleSplit\n",
        "- ê·¸ëƒ¥ train_test_splitì„ í•˜ë©´ labelë‹¹ ë°ì´í„°ê°€ ê³ ë¥´ê²Œ ì„ì´ì§€ ì•Šì„ ìˆ˜ê°€ ìˆë‹¤.\n",
        "- ê·¸ê²ƒì„ í”¼í•´ ê³ ë¥´ê²Œ ë¶„ë°°í•˜ê¸° ìœ„í•¨~\n",
        "```python\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkchB18abGyc"
      },
      "source": [
        "## ğŸ” 4. Data EDA (Exploratory Data Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6AJm-kdbGqV"
      },
      "source": [
        "- EDA (Exploratory Data Analysis, íƒìƒ‰ì  ìë£Œ ë¶„ì„): ë°ì´í„° ë¶„í¬ëŠ” ì–´ë–¤ì§€, ìƒê´€ê´€ê³„ëŠ” ì–´ë–¤ì§€, Outlierê°€ ìˆëŠ”ì§€, ë°ì´í„°ê°€ ê³¨ê³ ë£¨ ë¶„í¬í–ˆëŠ”ì§€ ê²°ì¸¡ì¹˜ê°€ ë§ì€ì§€ ë“±ë“±\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHVla3OfbG2m"
      },
      "source": [
        "## ğŸ” 5. Data FE (Feature Engineering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9i6TESYbG6e"
      },
      "source": [
        "- FEëŠ” êµ‰ì¥íˆ ë†’ì€ ì „ë¬¸ì„±ê³¼ ê²½í—˜ì´ í•„ìš”í•œ ë¶„ì•¼\n",
        "- FEì— ëŒ€í•´ì„œ ìì„¸í•˜ê²Œ ë°°ìš°ê³  ì‹¶ìœ¼ë©´ 2020-1 ë””ìì¸íŒ€ êµ¬ê¸€ë“œë¼ì´ë¸Œì— '[16ê¸°] ë””ìì¸íŒ€ íšŒì¹™ ë° ì»¤ë¦¬í˜ëŸ¼.docx'ì— ìˆëŠ” Udemy FEê°•ì˜ë¥¼ ì°¸ì¡°í•  ê²ƒ!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiAATuLcbG-b"
      },
      "source": [
        "### ğŸˆCategorical Data Hanlding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jHPdS8GIkuz"
      },
      "source": [
        "<h4> ğŸ“ Label encoding </h4> \n",
        "\n",
        "- ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ë¥¼ ë‹¤ë¥¸ ì •ìˆ«ê°’ìœ¼ë¡œ ë§¤í•‘í•´ì£¼ëŠ” ë°©ë²•\n",
        "- ['17í•™ë²ˆ','15í•™ë²ˆ', '17í•™ë²ˆ', '16í•™ë²ˆ', '19í•™ë²ˆ']ì„  \n",
        "  [0, 1, 0, 2, 3]ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ê²ƒ\n",
        "- Pandasì˜ factorize( ) ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ë¨.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
        "housing_cat_encoded[:10]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QA1AESgyyOu"
      },
      "source": [
        "<h4> ğŸ“ One-hot encoding </h4> \n",
        "\n",
        "- Label encodingì˜ ë‹¨ì ? ì»´í“¨í„°ê°€ ëª» ì•Œì•„ë¨¹ìŒ. ì»´í“¨í„° ì…ì¥ì—ì„œëŠ” Continuous Variableë¡œ ì¸ì‹\n",
        "- ì»´í“¨í„°ê°€ ì•Œì•„ë¨¹ì„ ìˆ˜ ìˆê²Œ í•˜ëŠ” ê²ƒì´ One-hot encoding!\n",
        "- ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆê²Œ 0, 1ë¡œë§Œ í‘œí˜„í•˜ëŠ” ê²ƒ  \n",
        "<br>\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile27.uf.tistory.com%2Fimage%2F999790455B7D3327107DBD)\n",
        "- Pandasì˜ get_dummies( ) ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ë¨.\n",
        "\n",
        "``` python\n",
        "# get_dummies() ì˜ˆì‹œ\n",
        "df_dum = pd.get_dummies(df['pclass'])\n",
        "\n",
        "df_train = pd.get_dummies(df_train, prefix = ['Cabin'], columns=['Cabin'])\n",
        "\n",
        "df_dum = pd.get_dummies(df)\n",
        "\n",
        "temp = pd.get_dummies(temp, columns = ['Age_cut'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61q0ZkG2bHJx"
      },
      "source": [
        "### ğŸˆScaling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX0iUQnYdMA-"
      },
      "source": [
        "> Scalingì´ë€.....\n",
        " - scalingì„ í†µí•´ì„œ ë‹¤ì°¨ì›ì˜ ê°’ë“¤ì„ ë¶„ì„í•˜ê¸° ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ê³  ìë£Œì˜ overflow, underflowë¥¼ ë°©ì§€í•  ìˆ˜ ìˆë‹¤! \n",
        " - ë˜í•œ ìµœì í™” ê³¼ì •ì—ì„œì˜ ì•ˆì •ì„± ë° ìˆ˜ë ´ì†ë„ë¥¼ í–¥ìƒ!\n",
        " - k-means, Gradient Descent ë“± ê±°ë¦¬ ê¸°ë°˜ì˜ ëª¨ë¸ì—ì„œëŠ” scalingì´ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤\n",
        " - Regression(Normal Equation), Tree based modelì€ Scalingì´ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "> ì¢…ë¥˜\n",
        " - StandardScaler: ê¸°ë³¸ ìŠ¤ì¼€ì¼, í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ì‚¬ìš©\n",
        " - MinMaxScaler: ìµœëŒ€/ìµœì†Œê°’ì´ ê°ê° 1, 0ì´ ë˜ë„ë¡ í•˜ëŠ” scaling\n",
        " - MaxAbsScaler: ìµœëŒ€ì ˆëŒ€ê°’ê³¼ 0ì´ ê°ê° 1, 0ì´ ë˜ë„ë¡ í•˜ëŠ” scaling\n",
        " - RobustScaler: medianê³¼ IQR(interquartile range) ì‚¬ìš©, outlierì˜ ì˜í–¥ì„ ìµœì†Œí™”í•œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mcFkUKN_JRv"
      },
      "source": [
        "# StandardScaler\n",
        "## ì˜ˆì œ1\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "standardScaler = StandardScaler()\n",
        "print(standardScaler.fit(train_data))\n",
        "train_data_standardScaled = standardScaler.transform(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXlr3ZWv_JRw"
      },
      "source": [
        "# MinMaxscaler\n",
        "## ì˜ˆì œ1\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "minMaxScaler = MinMaxScaler()\n",
        "print(minMaxScaler.fit(train_data))\n",
        "train_data_minMaxScaled = minMaxScaler.transform(train_data)\n",
        "\n",
        "## ì˜ˆì‹œ2\n",
        "minmax = MinMaxScaler()\n",
        "train_X_mm = minmax.fit_transform(train_X_nu)\n",
        "train_X_mm_df = pd.DataFrame(train_X_mm, columns=train_X_nu.columns, index=train_X_nu.index)\n",
        "train_X_fin = pd.concat([train_X_mm_df, train_X_ca], axis=1)\n",
        "train_X_fin.index = range(len(train_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr3_GEa2_JRx"
      },
      "source": [
        "# MaxAbsScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "maxAbsScaler = MaxAbsScaler()\n",
        "print(maxAbsScaler.fit(train_data))\n",
        "train_data_maxAbsScaled = maxAbsScaler.transform(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFKt_Vhi_JRz"
      },
      "source": [
        "# RobustScaler ì˜ˆì‹œ1\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "robustScaler = RobustScaler()\n",
        "print(robustScaler.fit(train_data))\n",
        "train_data_robustScaled = robustScaler.transform(train_data)\n",
        "\n",
        "# RobustScaler ì˜ˆì‹œ2_pandasì— ì ìš©í•˜ê¸°\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "temp[['psfMag_u', 'psfMag_g']] = scaler.fit_transform(temp[['psfMag_u', 'psfMag_g']])\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1SG_RNbIx5h"
      },
      "source": [
        "### ğŸˆSampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBQYOfXHc_NH"
      },
      "source": [
        "> Oversampling\n",
        " - RandomOverSampler: ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ë°ì´í„°ë¥¼ ë°˜ë³µí•´ì„œ ë„£ëŠ” ë°©ë²•\n",
        "``` python\n",
        "X_samp, y_samp = RandomOverSampler(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - ADASYN: ì†Œìˆ˜ í´ë˜ìŠ¤ ë°ì´í„°ì™€ ê·¸ ë°ì´í„°ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ kê°œì˜ ì†Œìˆ˜ í´ë˜ìŠ¤ ë°ì´í„° ì¤‘ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ë°ì´í„° ì‚¬ì´ì˜ ì§ì„ ìƒì— ê°€ìƒì˜ ì†Œìˆ˜ í´ë˜ìŠ¤ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ë°©ë²•\n",
        "```python\n",
        "X_samp, y_samp = ADASYN(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - SMOTE: ADASYN ë°©ë²•ì²˜ëŸ¼ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì§€ë§Œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ë¬´ì¡°ê±´ ì†Œìˆ˜ í´ë˜ìŠ¤ë¼ê³  í•˜ì§€ ì•Šê³  ë¶„ë¥˜ ëª¨í˜•ì— ë”°ë¼ ë¶„ë¥˜í•œë‹¤.\n",
        "```python\n",
        "X_samp, y_samp = SMOTE(random_state=4).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - SMOTENC: SMOTEì™€ ìœ ì‚¬í•˜ë‚˜ categorical ë³€ìˆ˜ë„ sampling ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•\n",
        "```python\n",
        "X_res, y_res = SMOTENC(random_state=42, categorical_features=[18, 19]).fit_resample(X, y)\n",
        "```\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "> Undersampling\n",
        " - **RandomUnderSampler**: ë¬´ì‘ìœ„ë¡œ ë°ì´í„°ë¥¼ ì—†ì• ëŠ” ë‹¨ìˆœ ìƒ˜í”Œë§\n",
        "```python\n",
        "  X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - **TomekLinks**: í† ë©•ë§í¬(í´ë˜ìŠ¤ê°€ ë‹¤ë¥¸ ë‘ ë°ì´í„° ì¤‘ ì•„ì£¼ ê°€ê¹Œì´ ë¶™ì–´ìˆëŠ” ë°ì´í„°)ë¥¼ ì°¾ì•„ì„œ ê·¸ ì¤‘ ë‹¤ìˆ˜ í´ë˜ìŠ¤ì— ì†í•œ ë°ì´í„°ë¥¼ ì œì™¸í•˜ëŠ” ë°©ë²•\n",
        "```python\n",
        "X_samp, y_samp = TomekLinks(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - **CondensedNearestNeighbour**: 1-NN ëª¨í˜•ìœ¼ë¡œ ë¶„ë¥˜ë˜ì§€ ì•ŠëŠ” ë°ì´í„°ë§Œ ë‚¨ê¸°ëŠ” ë°©ë²•\n",
        "```python\n",
        "X_samp, y_samp = CondensedNearestNeighbour(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - **OneSidedSelection**: í† ë§¥ë§í¬ ì¤‘ ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ë°ì´í„° ì¤‘ì—ì„œë„ ì„œë¡œ ë¶™ì–´ìˆëŠ” ë‹¤ìˆ˜ í´ë˜ìŠ¤ ë°ì´í„°ëŠ” 1-NN ë°©ë²•ìœ¼ë¡œ ì œì™¸í•œë‹¤.\n",
        "```python\n",
        "X_samp, y_samp = OneSidedSelection(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - **EditedNearestNeighbours**: ë‹¤ìˆ˜ í´ë˜ìŠ¤ ë°ì´í„° ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ k(n_neighbors)ê°œì˜ ë°ì´í„°ê°€ ëª¨ë‘(kind_sel=\"all\") ë˜ëŠ” ë‹¤ìˆ˜(kind_sel=\"mode\") ë‹¤ìˆ˜ í´ë˜ìŠ¤ê°€ ì•„ë‹ˆë©´ ì‚­ì œí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì†Œìˆ˜ í´ë˜ìŠ¤ ì£¼ë³€ì˜ ë‹¤ìˆ˜ í´ë˜ìŠ¤ ë°ì´í„°ëŠ” ì‚¬ë¼ì§„ë‹¤.\n",
        "```python\n",
        "X_samp, y_samp = EditedNearestNeighbours(kind_sel=\"all\", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - **NeighbourhoodCleaningRule**: CNN(Condensed Nearest Neighbour) ë°©ë²•ê³¼ ENN(Edited Nearest Neighbours) ë°©ë²•ì„ ì„ì€ ê²ƒì´ë‹¤.\n",
        "```python\n",
        "X_samp, y_samp = NeighbourhoodCleaningRule(kind_sel=\"all\", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "> ë³µí•©ìƒ˜í”Œë§\n",
        " - SMOTEENN: SMOTE + ENN\n",
        "```python\n",
        "X_samp, y_samp = SMOTEENN(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - SMOTETomek: SMOTE + Tomek\n",
        "```python\n",
        "X_samp, y_samp = SMOTETomek(random_state=4).fit_sample(X_imb, y_imb)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv9Uojbv_JR1"
      },
      "source": [
        "# SMOTE ì½”ë“œ\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)\n",
        "\n",
        "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
        "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
        "\n",
        "sm = SMOTE(random_state=2)\n",
        "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
        "\n",
        "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
        "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
        "\n",
        "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
        "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORd8tC4n_JR2"
      },
      "source": [
        "from imblearn.under_sampling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXEVSVj4_JR3"
      },
      "source": [
        "# sampling ì½”ë“œ ëª¨ìŒ\n",
        "X_samp, y_samp = RandomOverSampler(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = ADASYN(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = SMOTE(random_state=4).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = SMOTENC(random_state=42, categorical_features=[18, 19]).fit_resample(X_imb, y_imb)\n",
        "X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = TomekLinks(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = CondensedNearestNeighbour(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = OneSidedSelection(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = EditedNearestNeighbours(kind_sel=\"all\", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = NeighbourhoodCleaningRule(kind_sel=\"all\", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = SMOTEENN(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = SMOTETomek(random_state=4).fit_sample(X_imb, y_imb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlmT50pyI2Df"
      },
      "source": [
        "### ğŸˆPipeline Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aka-FTuMc4jg"
      },
      "source": [
        "> piplineì´ë€?\n",
        "- ì—°ì†ëœ ë³€í™˜ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í• ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” í´ë˜ìŠ¤ì„!\n",
        "- ì—°ì†ëœ ë‹¨ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì´ë¦„/estimator ìŒì˜ ëª©ë¡ì„ inputìœ¼ë¡œ ë°›ìŒ\n",
        "- ë§ˆì§€ë§‰ ë‹¨ê³„ì—ëŠ” estimator, transformer ëª¨ë‘ ì‚¬ìš©ê°€ëŠ¥í•˜ë©° ê·¸ ì™¸ì—ëŠ” ëª¨ë‘ estimator  \n",
        "<br>  \n",
        "\n",
        "> ColumnTransformerë€?\n",
        "- ë‘ pipelineì„ í•˜ë‚˜ì˜ pipelineìœ¼ë¡œ í•©ì¹˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guM-pJBr_JR5"
      },
      "source": [
        "# pipeline ì˜ˆì‹œ\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder()),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4DWvipV_JR7"
      },
      "source": [
        "# ColumnTransformer ì˜ˆì‹œ\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num_pipeline\", num_pipeline, num_attribs),\n",
        "        (\"cat_encoder\", OneHotEncoder(categories='auto'), cat_attribs),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL1W4FCFbND6"
      },
      "source": [
        "## ğŸ” 6. Select Model and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWRKCTwXbNHk"
      },
      "source": [
        "<h3> ğŸˆ Train and Evaluate Training Set </h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ii82vIIh1El"
      },
      "source": [
        "- Train dataë¡œ í›ˆë ¨ì„ í•œ ë‹¤ìŒì— Test dataë¡œ í‰ê°€ë¥¼ í•˜ë©´ ëœë‹¤!\n",
        "- í‰ê°€ ì§€í‘œëŠ” ìœ„ì—ì„œ ë‚˜ì˜¨ ì§€í‘œ ì¤‘ì—ì„œ í”„ë¡œì íŠ¸ì— ì œì¼ ì í•©í•œ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U0ZrMfpbNLo"
      },
      "source": [
        "<h3> ğŸˆEvaluation with Cross Validation </h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McV1bTkWxfr2"
      },
      "source": [
        "![datasets](https://t1.daumcdn.net/cfile/tistory/9951E5445AAE1BE025)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziYQkn1ExSNj"
      },
      "source": [
        "> Validation Setì´ ì™œ í•„ìš”í• ê¹Œ?\n",
        " - Validation Setì„ í†µí•´ ìš°ë¦¬ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëŒ€ëµì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŒ\n",
        " - Train Setì˜ ì¼ë¶€ë¥¼ ë–¼ì–´ë‚¸ í›„, ë‚¨ì€ ë¶€ë¶„ì„ í•™ìŠµí•œ ë’¤, ëª¨ë¸ì„ í†µí•´ ë–¼ì–´ë‚¸ ë¶€ë¶„ì— ëŒ€í•œ ì˜ˆì¸¡ ì§„í–‰!\n",
        " - ìš°ë¦¬ëŠ” ë–¼ì–´ë‚¸ ë¶€ë¶„ì˜ ì‹¤ì œ ê°’ì„ ì•Œê³  ìˆê¸° ë•Œë¬¸ì— ì˜ˆì¸¡ì¹˜ì™€ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì—¬ëŸ¬ ì¸¡ì • ê³µì‹ì„ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•¨\n",
        " -  ê·¸ëŸ¬ë‹ˆ Validation Setì€ ëª¨ì˜ê³ ì‚¬ ë¬¸ì œì¸ ì…ˆì´ì¥¬.  \n",
        " <br>\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt43DS3HdqRx"
      },
      "source": [
        "<h4> Cross Validation </h4>\n",
        "\n",
        "- Cross validationì€ validation setì„ ë—„ ë§Œí¼ dataê°€ í¬ì§€ ì•Šì„ ë•Œ ì‚¬ìš©\n",
        "- Training Setì„ ì—¬ëŸ¬ Subsetìœ¼ë¡œ ë‚˜ëˆ„ê³  ê° ëª¨ë¸ì„ ì´ Subsetì˜ ì¡°í•©ìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ê³  ë‚˜ë¨¸ì§€ ë¶€ë¶„ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” ë°©ë²•ì´ë‹¤.  \n",
        "![validation](https://t1.daumcdn.net/cfile/tistory/990DD2465B72F1491E)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2C7OG7O_JR_"
      },
      "source": [
        "# Cross validation code\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "X = np.arange(16).reshape((8,-1)) ## 8ê°œì˜ row, 2ê°œì˜ column\n",
        "y = np.arange(8).reshape((-1,1))\n",
        "kf = KFold(n_splits=4) # 8ê°œì˜ rowë¥¼ ê°ê° 2ì¤„ì§œë¦¬ 4ê°œì˜ setìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index] \n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "# ê°ê°ì˜ iterationì„ printí•©ë‹ˆë‹¤. ì²« ì¤„ì˜ ì˜ë¯¸ëŠ” 2~7ë²ˆì§¸ rowë¥¼ í•™ìŠµí•˜ê³  0,1ë²ˆì§¸ rowë¥¼ validation setìœ¼ë¡œ í•˜ì—¬ ê²€ì¦í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWVeBDgx_JSB"
      },
      "source": [
        "# Kfold ì˜ˆì‹œ1\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10)\n",
        "lasso_regressor = Lasso()\n",
        "ridge_regressor = Ridge()\n",
        "\n",
        "lasso_mse = []\n",
        "ridge_mse = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    lasso_regressor.fit(X[train_index], y[train_index])\n",
        "    ridge_regressor.fit(X[train_index], y[train_index])\n",
        "    \n",
        "    lasso_mse.append(mean_squared_error(y[test_index], lasso_regressor.predict(X[test_index])))\n",
        "    ridge_mse.append(mean_squared_error(y[test_index], ridge_regressor.predict(X[test_index])))\n",
        "    \n",
        "sum(lasso_mse) / 10, sum(ridge_mse) / 10\n",
        "\n",
        "################################################################################################################\n",
        "\n",
        "# Kfold ì˜ˆì‹œ2\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np \n",
        "\n",
        "lasso_regressor = Lasso(warm_start=False)\n",
        "ridge_regressor = Ridge()\n",
        "\n",
        "lasso_scores = cross_val_score(lasso_regressor, X, y, cv=10, \n",
        "                               scoring='neg_mean_squared_error')\n",
        "ridge_scores= cross_val_score(ridge_regressor, X, y, cv=10, \n",
        "                              scoring='neg_mean_squared_error')\n",
        "# cv: ëª‡ ë²ˆ ëŒë¦´ ê²ƒì´ëƒ. cross-validation, scoring: ì ìˆ˜ë¥¼ ì¸¡ì •í•˜ëŠ” ê¸°ì¤€\n",
        "np.mean(lasso_scores), np.mean(ridge_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ggGGOObNPQ"
      },
      "source": [
        "## ğŸ” 7. Model Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUN69aHajLbb"
      },
      "source": [
        "- ëª¨ë¸ì´ ë³µì¡í•´ì§€ë©´ ë³µì¡í•´ì§ˆìˆ˜ë¡ ìˆ˜ë§ì€ hyperparameterë“¤ì´ ìˆë‹¤.\n",
        "- Sklearnì˜ LinearRegression( )ì€ 4ê°œì˜ hyperparameterê°€ ìˆëŠ” ë°˜ë©´ XgboostëŠ” 40ì—¬ê°œì˜ hyperparameterë“¤ì´ ìˆë‹¤.\n",
        "- í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œì— ë§ëŠ” ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” Hyperparameterë¥¼ ì˜ ì¡°ì •í•´ì£¼ë©´ ëœë‹¤!\n",
        "- Hyperparameterë¥¼ ì°¾ëŠ” ë°©ë²•ì€ í¬ê²Œ 2ê°€ì§€ê°€ ìˆë‹¤.\n",
        "  - GridSearchCV\n",
        "  - RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXxqASE8bNTj"
      },
      "source": [
        "### ğŸˆGridSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR3nDm1P_JSE"
      },
      "source": [
        "> GridSearchCVë€?\n",
        "- ëª¨ë¸ì˜ hyperparameterë¥¼ ì°¾ëŠ” ê²ƒì„ ë„ì™€ì£¼ëŠ” ì¹œêµ¬\n",
        "\n",
        "\n",
        "> ë©”ì†Œë“œ ëª¨ìŒ\n",
        "- **GridSearchCV ( estimator = , param_grid = , scoring = , cv = , n_jobs = , verbose = )** : grid search í•˜ê¸°\n",
        "- **.best_scores_**: ìµœì ì˜ score ì ìˆ˜ë¥¼ ë³´ì—¬ì¤Œ\n",
        "- **.best_estimator_**: ìµœì ì˜ parameterë¡œ ì„¤ì •ëœ ëª¨ë¸ì„ ìƒì„±\n",
        "- **.best_params_**: ìµœì ì˜ parameterë¥¼ ë°˜í™˜\n",
        "- **.cv_results_**: ì „ì²´ì ì¸ ê²°ê³¼ê°’ë“¤ì„ ë³´ì—¬ì¤Œ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ovVFOM-_JSF"
      },
      "source": [
        "## Catboost best parameter ì°¾ëŠ” ê³¼ì •\n",
        "# ìš°ì„  ë¶€ì°¨ì ì¸ parameter ì°¾ì•„ì£¼ê³ \n",
        "cb = CatBoostClassifier(\n",
        " learning_rate =0.1,\n",
        " iterations=100 #n-estimatorëŒ€ì‹  iterationì„ ì‚¬ìš©,\n",
        ")\n",
        "\n",
        "cb_params_1 = {\n",
        "    'depth' : [3,5,7],\n",
        "    'random_strength' : [1,3],\n",
        "    'bagging_temperature' : [0,0.5,1],\n",
        "    'l2_leaf_reg' : [1,3,5,7],\n",
        "}\n",
        "cb_grid_1 = GridSearchCV(cb, param_grid=cb_params_1, scoring=my_scorer, cv=5, verbose=1)\n",
        "cb_grid_1.fit(train[features], train['Survived'])\n",
        "\n",
        "print(\"Best Score : {}\".format(cb_grid_1.best_score_))\n",
        "print(\"Best Params : {}\".format(cb_grid_1.best_params_))\n",
        "best_cb_model = cb_grid_1.best_estimator_\n",
        "\n",
        "# ìµœì ì˜ core parameter ì°¾ê¸°\n",
        "cb_params_2 = {\n",
        "    'learning_rate' : [0.03, 0.07, 0.1],\n",
        "    'iterations' : [n for n in range(80,130,20)]\n",
        "}\n",
        "cb_grid_2 = GridSearchCV(best_cb_model, param_grid=cb_params_2, scoring=my_scorer, cv=5, verbose=1)\n",
        "cb_grid_2.fit(train[features], train['Survived'])\n",
        "\n",
        "print(\"Best Score : {}\".format(cb_grid_2.best_score_))\n",
        "print(\"Best Params : {}\".format(cb_grid_2.best_params_))\n",
        "best_cb_model = cb_grid_2.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ItofVWa_JSG"
      },
      "source": [
        "### Xgboost best parameter ì°¾ê¸°\n",
        "## ì²˜ìŒì—ëŠ” ë¶€ì°¨ì ì¸ parameterë¥¼ tuningí•˜ê¸°\n",
        "# GridSearchCVì— ë“¤ì–´ê°ˆ param_grid, estimator, scoring ë§Œë“¤ì–´ì£¼ê¸°\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import auc, f1_score, accuracy_score\n",
        "my_scorer = make_scorer(accuracy_score, greater_is_better = True)\n",
        "\n",
        "xgb1 = XGBClassifier(\n",
        " learning_rate =0.1,\n",
        " n_estimators=100,\n",
        ")\n",
        "\n",
        "xgb_params_1 = {\n",
        "    'max_depth' : [3,5,7],\n",
        "    'min_child_weight' : [0.5, 1],\n",
        "    'gamma' : [0, 0.1],\n",
        "    'subsample' : [0.5, 0.7, 0.9],\n",
        "    'colsample_bytree' : [0.5, 0.7, 0.9],\n",
        "} #3x2x2x3x3=108ê°€ì§€ ê²½ìš°ì˜ ìˆ˜\n",
        "\n",
        "# GridSearchCV ëŒë¦¬ê¸°\n",
        "xgb_grid_1 = GridSearchCV(estimator=xgb1, param_grid=xgb_params_1, \n",
        "                          scoring=my_scorer, cv=5, n_jobs=1, verbose=1)\n",
        "xgb_grid_1.fit(train[features], train['Survived']) # Titanic data\n",
        "\n",
        "# ì œì¼ ì¢‹ì€ ëª¨ë¸ ë½‘ê¸°\n",
        "print(\"Best Score : {}\".format(xgb_grid_1.best_score_))\n",
        "print(\"Best Params : {}\".format(xgb_grid_1.best_params_))\n",
        "\n",
        "# í‘œë¡œ í•œ ë²ˆ ë½‘ì•„ë³´ê³ \n",
        "results = pd.DataFrame(xgb_grid_1.cv_results_)\n",
        "results = results.sort_values(by='mean_test_score', ascending=False)\n",
        "results.head()\n",
        "\n",
        "# ë‹¤ì‹œ Core parameter ì„¤ì •!\n",
        "best_xgb_model = xgb_grid_1.best_estimator_\n",
        "xgb_params_2 = {\n",
        "    'learning_rate' : [0.01, 0.05, 0.07, 0.1, 0.2],\n",
        "    'n_estimators' : [n for n in range(100,200,20)]\n",
        "}\n",
        "xgb_grid_2 = GridSearchCV(best_xgb_model, param_grid=xgb_params_2, scoring=my_scorer, cv=5, verbose=1)\n",
        "xgb_grid_2.fit(train[features], train['Survived'])\n",
        "\n",
        "# ì œì¼ ì¢‹ì€ ëª¨ë¸ ë½‘ê¸°\n",
        "print(\"Best Score : {}\".format(xgb_grid_2.best_score_))\n",
        "print(\"Best Params : {}\".format(xgb_grid_2.best_params_))\n",
        "best_xgb_model = xgb_grid_2.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ghfBIcdbNXn"
      },
      "source": [
        "### ğŸˆRandomizedSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j419E81O_JSJ"
      },
      "source": [
        "> RandomizedSearchCVë€?\n",
        "- GridSearchCVëŠ” ë¹„êµì  ì ì€ ìˆ˜ì˜ ì¡°í•©ì„ íƒêµ¬í•  ë•Œ ê´œì°®ìŒ\n",
        "- ë‹¨ hyperparameter íƒìƒ‰ ê³µê°„ì´ ì»¤ì§ˆ ë•ŒëŠ” RandomizedSearchCVê°€ ë” ìœ ìš©\n",
        ">> ì¥ì \n",
        "  - íƒìƒ‰ 1000íšŒ -> ì„œë¡œ ë‹¤ë¥¸ hyperparameter ì¡°í•© 1000ê°œ ê°’ íƒìƒ‰\n",
        "  - ë‹¨ìˆœíˆ ë°˜ë³µ íšŸìˆ˜ ì¡°ì ˆí•˜ëŠ” ê²ƒìœ¼ë¡œë„ ì»´í“¨íŒ… ìì› ì œì–´ê°€ëŠ¥"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euVR9L3D_JSJ"
      },
      "source": [
        "### RandomizedSearchCV ì˜ˆì‹œ 1\n",
        "# specify param\\eters and distributions to sample from\n",
        "param_dist = {'average': [True, False],\n",
        "              'l1_ratio': stats.uniform(0, 1),\n",
        "              'alpha': loguniform(1e-4, 1e0)}\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search)\n",
        "random_search.fit(X, y)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpietYbT_JSK"
      },
      "source": [
        "### RandomizedSearchCV ì˜ˆì‹œ 2\n",
        "clf = BayesianRidge()\n",
        "clf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
        "#clf = RandomForestRegressor()\n",
        "if classifier:\n",
        "  clf = classifier\n",
        "  to_tune = False\n",
        "if to_tune:\n",
        "  # Grid search: find optimal classifier parameters.\n",
        "  param_grid = {'alpha_1': sp_rand(), 'alpha_2': sp_rand()}\n",
        "  param_grid = {'C': sp_rand(), 'gamma': sp_rand()}\n",
        "  rsearch = RandomizedSearchCV(estimator=clf, \n",
        "  param_distributions=param_grid, n_iter=5000)\n",
        "  rsearch.fit(X_train, y_train)\n",
        "  # Use tuned classifier.\n",
        "  clf = rsearch.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D2RtFha_JSM"
      },
      "source": [
        "### RandomizedSearchCV ì˜ˆì‹œ 3\n",
        "n_estimators = [100, 200, 300, 400, 500]\n",
        "max_features = ['auto', 'sqrt']\n",
        "max_depth = [5, 10, 20, 30, 40, 50]\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        " model_tuned = RandomizedSearchCV(estimator = baseModel, \n",
        "                                  param_distributions = random_grid, \n",
        "                                  n_iter = 2, cv = 2, verbose=0, \n",
        "                                  random_state=42 , n_jobs = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCYcjIVPIYNC"
      },
      "source": [
        "### ğŸˆ Bayesian optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSFEenxvJBhM"
      },
      "source": [
        "- ë¶ˆí•„ìš”í•œ hyperparameter íƒìƒ‰ì‹œê°„ì„ ì¤„ì—¬ ë³´ë‹¤ ë¹ ë¥´ê²Œ ìµœì  hyperparameterë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜\n",
        "- GridSearchCV, RandomizedSearchCV ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ì•Œê³ ë¦¬ì¦˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WolWZ4X6ppn8"
      },
      "source": [
        "#### ğŸ“ Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMaunQSGZtxT"
      },
      "source": [
        "<h5>ğŸ“Œ BayesianOptimization with Logistic Regression <h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr1FJTjyZkYy"
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def logreg_cv(C,\n",
        "          max_iter,\n",
        "          x_data=None, y_data=None, n_splits=5, output='score'):\n",
        "    \n",
        "    # K Fold êµì°¨ê²€ì¦ ë§Œë“¤ê¸°\n",
        "    score = 0\n",
        "    kf = KFold(n_splits=n_splits, random_state=4321)\n",
        "    models = []\n",
        "    for train_index, valid_index in kf.split(x_data):\n",
        "        x_train, y_train = x_data[train_index], y_data[train_index]\n",
        "        x_valid, y_valid = x_data[valid_index], y_data[valid_index]\n",
        "\n",
        "    # ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "        model = LogisticRegression(\n",
        "            C = C,\n",
        "            max_iter = int(max_iter)\n",
        "        )\n",
        "        \n",
        "        model.fit(x_train, y_train)\n",
        "        models.append(model)\n",
        "        \n",
        "    # Evaluate model with average f1 score    \n",
        "        pred = model.predict(x_valid)\n",
        "        true = y_valid\n",
        "        score += accuracy_score(true, pred)/n_splits\n",
        "    \n",
        "    if output == 'score':\n",
        "        return score\n",
        "    if output == 'model':\n",
        "        return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDyGIyn2oHNW"
      },
      "source": [
        "<h5> ğŸ“Œ BayesianOptimization with XGBoost (Binary Class) </h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvokIum-oJvI"
      },
      "source": [
        "def XGB_CV(max_depth,\n",
        "           gamma,\n",
        "           min_child_weight,\n",
        "           max_delta_step,\n",
        "           subsample,\n",
        "           colsample_bytree,\n",
        "           learning_rate\n",
        "         ):\n",
        "    global AUCbest\n",
        "    global ITERbest\n",
        "    \n",
        "    folds = 10\n",
        "    cv_score = 0\n",
        "    \n",
        "    # Model defining\n",
        "    xgb = XGBClassifier(max_depth = int(max_depth),\n",
        "                        gamma = gamma,\n",
        "                        learning_rate = learning_rate,\n",
        "                        subsample = max(min(subsample, 1), 0),\n",
        "                        colsample_bytree = max(min(colsample_bytree, 1), 0),\n",
        "                        min_child_weight = min_child_weight,\n",
        "                        max_delta_step = int(max_delta_step),\n",
        "                        n_estimators = 20000,\n",
        "                        random_state=42, \n",
        "#                         tree_method='gpu_hist' ,\n",
        "                       )\n",
        "    \n",
        "    # Model Training\n",
        "    xgb.fit(X_train, y_train,\n",
        "            early_stopping_rounds = 100,\n",
        "            eval_metric=[\"auc\"], verbose=False,\n",
        "            eval_set=[(X_test, y_test)])\n",
        "    \n",
        "    \n",
        "    val_score = max(xgb.evals_result()['validation_0']['auc'])\n",
        "    print(' Stopped after %d iterations with val-auc = %f val-gini = %f' % ( len(xgb.evals_result()['validation_0']['auc']), val_score, (val_score*2-1)) )\n",
        "    if ( val_score > AUCbest ):\n",
        "        AUCbest = val_score\n",
        "        ITERbest = len(xgb.evals_result()['validation_0']['auc'])\n",
        "\n",
        "    return (val_score*2) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4uOHWLfoJsX"
      },
      "source": [
        "X = titanic.iloc[:, 1:]; y = titanic.iloc[:, 0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfcwJ2jNoJpj"
      },
      "source": [
        "XGB_BO = BayesianOptimization(XGB_CV,pbounds= {\n",
        "                                    'max_depth': (2, 12),\n",
        "                                    'gamma': (0.001, 10.0),\n",
        "                                    'min_child_weight': (0, 20),\n",
        "                                    'max_delta_step': (0, 10),\n",
        "                                    'subsample': (0.4, 1.0),\n",
        "                                    'colsample_bytree' :(0.4, 1.0),\n",
        "                                    'learning_rate' : (0.01, 0.1), \n",
        "                                    }, \n",
        "                               verbose= 2,\n",
        "                               random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHe0HmH5oJml"
      },
      "source": [
        "AUCbest = -1.\n",
        "ITERbest = 0\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    XGB_BO.maximize(init_points=2, n_iter=30, acq='ei', xi=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh1N8qcHoJjj"
      },
      "source": [
        "# Best hyperparameter\n",
        "print(XGB_BO.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6omOliWZz11"
      },
      "source": [
        "<h5>ğŸ“Œ  BayesianOptimization with XGBoost (Multiclass) </h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwGYcThtoDNC"
      },
      "source": [
        "def custom_eval(pred, dtrain):\n",
        "  labels = dtrain.get_label()\n",
        "  lb = LabelBinarizer()\n",
        "  lb.fit(labels)\n",
        "  label = lb.transform(labels)\n",
        "  return 'roc_auc' , -roc_auc_score(label, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46sud6xHZsNg"
      },
      "source": [
        "def XGB_CV(max_depth,\n",
        "           gamma,\n",
        "           min_child_weight,\n",
        "           max_delta_step,\n",
        "           subsample,\n",
        "           colsample_bytree,\n",
        "           learning_rate,\n",
        "         ):\n",
        "    global AUCbest\n",
        "    global ITERbest\n",
        "\n",
        "    # Model defining\n",
        "    xgb = XGBClassifier(max_depth = int(max_depth),\n",
        "                        gamma = gamma,\n",
        "                        learning_rate = learning_rate,\n",
        "                        subsample = max(min(subsample, 1), 0),\n",
        "                        colsample_bytree = max(min(colsample_bytree, 1), 0),\n",
        "                        min_child_weight = min_child_weight,\n",
        "                        max_delta_step = int(max_delta_step),\n",
        "                        n_estimators = 20000,\n",
        "                        random_state=42, \n",
        "#                         tree_method='gpu_hist' ,\n",
        "                        silent=True)\n",
        "    \n",
        "    # Model Training\n",
        "    xgb.fit(X_train, y_train,\n",
        "            early_stopping_rounds = 100,\n",
        "            eval_set=[(X_test, y_test)], \n",
        "            eval_metric=custom_eval, verbose=False)\n",
        "    \n",
        "    val_score = -xgb.evals_result()['validation_0']['roc_auc'][-1]\n",
        "    print(' Stopped after %d iterations with val-auc = %f val-gini = %f' % ( len(xgb.evals_result()['validation_0']['roc_auc']), val_score, (val_score*2-1)) )\n",
        "    if ( val_score > AUCbest ):\n",
        "        AUCbest = val_score\n",
        "        ITERbest = len(xgb.evals_result()['validation_0']['roc_auc'])\n",
        "\n",
        "    return (val_score*2) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBdu4qnEoDt0"
      },
      "source": [
        "XGB_BO = BayesianOptimization(XGB_CV,pbounds= {\n",
        "                                    'max_depth': (2, 12),\n",
        "                                    'gamma': (0.001, 10.0),\n",
        "                                    'min_child_weight': (0, 20),\n",
        "                                    'max_delta_step': (0, 10),\n",
        "                                    'subsample': (0.4, 1.0),\n",
        "                                    'colsample_bytree' :(0.4, 1.0),\n",
        "                                    'learning_rate' : (0.01, 0.1),                                  \n",
        "                                    }, \n",
        "                               verbose= 2,\n",
        "                               random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTq_LObUoFIE"
      },
      "source": [
        "X = df_nn.iloc[:, :-1].values; y = df_nn.iloc[:, -1].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpRqFQwQoFNK"
      },
      "source": [
        "AUCbest = -1.\n",
        "ITERbest = 0\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    XGB_BO.maximize(init_points=2, n_iter=30, acq='ei', xi=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTIvSx2noFSS"
      },
      "source": [
        "# Best hyperparameter\n",
        "print(XGB_BO.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcfcRVzlp_s5"
      },
      "source": [
        "<h5>ğŸ“Œ  BayesianOptimization with LightGBM (Binary Class) </h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se1_eLcAqDe_"
      },
      "source": [
        "def LGB_opt(max_depth,\n",
        "           gamma,\n",
        "           min_child_weight,\n",
        "           max_delta_step,\n",
        "           subsample,\n",
        "           colsample_bytree,\n",
        "           learning_rate,\n",
        "         ):\n",
        "\n",
        "    global AUCbest\n",
        "    global ITERbest\n",
        "\n",
        "    folds = 10\n",
        "    cv_score = 0\n",
        "\n",
        "    lgb = LGBMClassifier(class_weight=None,\n",
        "                        colsample_bytree = max(min(colsample_bytree, 1), 0), \n",
        "                        learning_rate = learning_rate,\n",
        "                        #gamma = gamma, \n",
        "                        max_depth = int(max_depth), \n",
        "                        min_child_weight = min_child_weight, \n",
        "                        subsample = max(min(subsample, 1), 0),\n",
        "                        num_leaves = min(2**int(max_depth),131072),\n",
        "                        n_estimators = 20000, \n",
        "                        n_jobs=-1, \n",
        "#                         device='gpu',\n",
        "                        random_state=42)\n",
        "                        \n",
        "    lgb.fit(X_train, y_train,\n",
        "            verbose=False,  \n",
        "            early_stopping_rounds = 100,\n",
        "            eval_set=[(X_test, y_test)],\n",
        "            eval_metric=['auc'])\n",
        "\n",
        "    val_score = lgb.evals_result_['valid_0']['auc'][-1]\n",
        "    print(' Stopped after %d iterations with val-auc = %f val-gini = %f' % ( len(lgb.evals_result_['valid_0']['auc']), val_score, (val_score*2-1)) )\n",
        "    if ( val_score > AUCbest ):\n",
        "        AUCbest = val_score\n",
        "        ITERbest = len(lgb.evals_result_['valid_0']['auc'])\n",
        "\n",
        "    return (val_score*2) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA-a3nwBqDlU"
      },
      "source": [
        "X = titanic.iloc[:, 1:]; y = titanic.iloc[:, 0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddlzzQTQqDiN"
      },
      "source": [
        "LGB_BO = BayesianOptimization(LGB_opt, {\n",
        "                                    'max_depth': (2, 20),\n",
        "                                    'gamma': (0.001, 10.0),\n",
        "                                    'min_child_weight': (0, 20),\n",
        "                                    'max_delta_step': (0, 10),\n",
        "                                    'subsample': (0.4, 1.0),\n",
        "                                    'colsample_bytree' :(0.4, 1.0),\n",
        "                                    'learning_rate' : (0.01, 0.1)\n",
        "                                    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ghd9NS1qDoB"
      },
      "source": [
        "AUCbest = -1.\n",
        "ITERbest = 0\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    LGB_BO.maximize(init_points=2, n_iter=30, acq='ei', xi=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdRF62gLqUaQ"
      },
      "source": [
        "print(LGB_BO.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgpc6KkFp_wD"
      },
      "source": [
        "<h5>ğŸ“Œ  BayesianOptimization with LightGBM (Multiclass) </h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGO1AoxGqHQh"
      },
      "source": [
        "def LGB_opt(max_depth,\n",
        "           gamma,\n",
        "           min_child_weight,\n",
        "           max_delta_step,\n",
        "           subsample,\n",
        "           colsample_bytree,\n",
        "           learning_rate,\n",
        "         ):\n",
        "\n",
        "    global AUCbest\n",
        "    global ITERbest\n",
        "\n",
        "    folds = 10\n",
        "    cv_score = 0\n",
        "\n",
        "    lgb = LGBMClassifier(class_weight=None,\n",
        "                        colsample_bytree = max(min(colsample_bytree, 1), 0), \n",
        "                        learning_rate = learning_rate,\n",
        "                        #gamma = gamma, \n",
        "                        max_depth = int(max_depth), \n",
        "                        min_child_weight = min_child_weight, \n",
        "                        subsample = max(min(subsample, 1), 0),\n",
        "                        num_leaves = min(2**int(max_depth),131072),\n",
        "                        n_estimators = 20000, \n",
        "                        n_jobs=-1, \n",
        "                        device='gpu',\n",
        "                        random_state=42)\n",
        "                        \n",
        "    lgb.fit(X_train, y_train,\n",
        "            verbose=False,  \n",
        "            early_stopping_rounds = 100,\n",
        "            eval_set=[(X_test, y_test)],\n",
        "            eval_metric=custom_eval)\n",
        "\n",
        "    val_score = lgb.evals_result_['valid_0']['roc_auc'][-1]\n",
        "    print(' Stopped after %d iterations with val-auc = %f val-gini = %f' % ( len(lgb.evals_result_['valid_0']['roc_auc']), val_score, (val_score*2-1)) )\n",
        "    if ( val_score > AUCbest ):\n",
        "        AUCbest = val_score\n",
        "        ITERbest = len(lgb.evals_result_['valid_0']['roc_auc'])\n",
        "\n",
        "    return (val_score*2) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFc9biywqHTm"
      },
      "source": [
        "def custom_eval(y_true, y_pred):\n",
        "  preds = y_pred.reshape(11, -1).T\n",
        "  lb = LabelBinarizer()\n",
        "  lb.fit(y_true)\n",
        "  label = lb.transform(y_true)\n",
        "  return 'roc_auc' , roc_auc_score(label, preds), True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me21tnUgqHXQ"
      },
      "source": [
        "LGB_BO = BayesianOptimization(LGB_opt, {\n",
        "                                    'max_depth': (2, 20),\n",
        "                                    'gamma': (0.001, 10.0),\n",
        "                                    'min_child_weight': (0, 20),\n",
        "                                    'max_delta_step': (0, 10),\n",
        "                                    'subsample': (0.4, 1.0),\n",
        "                                    'colsample_bytree' :(0.4, 1.0),\n",
        "                                    'learning_rate' : (0.01, 0.1)\n",
        "                                    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPRRU-zvqK4U"
      },
      "source": [
        "df = pd.read_csv(path+test_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe5dKYJbqHaT"
      },
      "source": [
        "X = df.iloc[:, 1:].values; y = df.iloc[:, 0].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rczS8eDIqHdK"
      },
      "source": [
        "AUCbest = -1.\n",
        "ITERbest = 0\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    LGB_BO.maximize(init_points=2, n_iter=30, acq='ei', xi=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESQeSpcGqHf2"
      },
      "source": [
        "print(LGB_BO.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJukgb2bNbQ"
      },
      "source": [
        "<h3> ğŸˆ Ensemble </h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcVpF-pzms4N"
      },
      "source": [
        "- \"ì•½í•œ ëª¨ë¸ì„ ì—¬ëŸ¬ ê°œ í•©ì¹˜ë©´ ê°•í•œ ëª¨ë¸ 1ê°œë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ë‹¤!\"\n",
        "- ì´ê²Œ ì•™ìƒë¸”ì˜ ì•„ì´ë””ì–´!\n",
        "- 5ì›”ì— ë°°ìš°ê²Œ ë  ê±°ì˜ˆìš”!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYSuxKNWbNfE"
      },
      "source": [
        "<h3> ğŸˆ Best Model and Error Analysis </h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPSEN3UOnDVq"
      },
      "source": [
        "- ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ë¶„ì„í•˜ë©´ ë¬¸ì œì— ëŒ€í•œ ì¢‹ì€ insightë“¤ì„ ë§ì´ ì–»ê¸°ë„ í•œë‹¤.\n",
        "- ì˜ˆ) Feature_importances_ë¥¼ ì‚¬ìš©í•˜ë©´ ì–´ë–¤ featureê°€ ëª¨ë¸ì— ì˜í–¥ì„ ë§ì´ ë¯¸ì¹˜ëŠ”ì§€ ì•Œë ¤ì¤€ë‹¤! ì¦‰ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œì—ì„œ ì–´ë–¤ íŠ¹ì„±ë“¤ì´ ì¤‘ìš”í•œì§€ ìƒëŒ€ì  ì¤‘ìš”ë„ë¥¼ ì•Œë ¤ì¤€ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCdGnJyUnalB"
      },
      "source": [
        "## Random Forest Feature importance\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
        "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "    print(name, score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49rmjjecnaUn"
      },
      "source": [
        "## Xgboost Plot importance\n",
        "# ì½”ë“œ1\n",
        "from xgboost import plot_importance\n",
        "\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X, y)\n",
        "sorted_idx = np.argsort(model.feature_importances_)[::-1]\n",
        "for index in sorted_idx:\n",
        "    print([X.columns[index], model.feature_importances_[index]])\n",
        "\n",
        "# ì½”ë“œ2\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "model = XGBClassifier()\n",
        "model.fit(train, label)\n",
        "\n",
        "sorted_idx = np.argsort(model.feature_importances_)[::-1]\n",
        "\n",
        "for index in sorted_idx:\n",
        "    print([train.columns[index], model.feature_importances_[index]]) \n",
        "\n",
        "    plot_importance(model, max_num_features = 15)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEzSnmlTbNjN"
      },
      "source": [
        "<h3> ğŸˆ Evaluate System with Test Set </h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiUxy8_jn5Nw"
      },
      "source": [
        "- Train setê³¼ Validation setìœ¼ë¡œ í›ˆë ¨ì‹œí‚¨ ëª¨ë¸ì„ ì´ì œ Test setì— ì ìš©í•´ì„œ scoreë¥¼ ë³´ë©´ ëœë‹¤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBQ_pbhmbTXV"
      },
      "source": [
        "## ğŸ” 8. Launch, Monitoring, and System Maintenance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAL0mzN7oE4V"
      },
      "source": [
        "- ì´ì œ í•´ë‹¹ ëª¨ë¸ì„ ì ìš©ì‹œì¼œì„œ ë¬¸ì œ í•´ê²°ì— ì ê·¹ í™œìš©í•˜ë©´ ëœë‹¤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJWmUSDlAyOK"
      },
      "source": [
        "# âœï¸ Useful Machine Learning Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjjuHB0rwULR"
      },
      "source": [
        "<h3> ğŸ” How to Save Models</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_W-Sdk6w8vO"
      },
      "source": [
        "Use **joblib**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZLxjgBNOXCf"
      },
      "source": [
        "## ì €ì¥í•  ë•Œ\n",
        "from sklearn.externals import joblib \n",
        "# ê°ì²´ë¥¼ pickled binary file í˜•íƒœë¡œ ì €ì¥í•œë‹¤ \n",
        "file_name = 'object_01.pkl' \n",
        "joblib.dump(obj, file_name) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlqZIRBCwWwI"
      },
      "source": [
        "## ì½ì„ ë•Œ\n",
        "from sklearn.externals import joblib \n",
        "# pickled binary file í˜•íƒœë¡œ ì €ì¥ëœ ê°ì²´ë¥¼ ë¡œë”©í•œë‹¤ \n",
        "file_name = 'object_01.pkl' \n",
        "obj = joblib.load(file_name) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_CciL_ew_OG"
      },
      "source": [
        "Using **pickle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvMC4llKwA2_"
      },
      "source": [
        "# Save Model Using Pickle\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "\n",
        "dataframe = pandas.read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "test_size = 0.33\n",
        "seed = 7\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Fit the model on training set\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        " \n",
        "# some time later...\n",
        " \n",
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.score(X_test, Y_test)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}